<!DOCTYPE html><html><head>
      <title>第二章</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      
        <script type="text/javascript">
          window.MathJax = ({"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"options":{},"loader":{}});
        </script>
        <script type="text/javascript" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
      
      
      
      
      
      <style>
      code[class*=language-],pre[class*=language-]{color:#333;background:0 0;font-family:Consolas,"Liberation Mono",Menlo,Courier,monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.4;-moz-tab-size:8;-o-tab-size:8;tab-size:8;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}pre[class*=language-]{padding:.8em;overflow:auto;border-radius:3px;background:#f5f5f5}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal;background:#f5f5f5}.token.blockquote,.token.comment{color:#969896}.token.cdata{color:#183691}.token.doctype,.token.macro.property,.token.punctuation,.token.variable{color:#333}.token.builtin,.token.important,.token.keyword,.token.operator,.token.rule{color:#a71d5d}.token.attr-value,.token.regex,.token.string,.token.url{color:#183691}.token.atrule,.token.boolean,.token.code,.token.command,.token.constant,.token.entity,.token.number,.token.property,.token.symbol{color:#0086b3}.token.prolog,.token.selector,.token.tag{color:#63a35c}.token.attr-name,.token.class,.token.class-name,.token.function,.token.id,.token.namespace,.token.pseudo-class,.token.pseudo-element,.token.url-reference .token.variable{color:#795da3}.token.entity{cursor:help}.token.title,.token.title .token.punctuation{font-weight:700;color:#1d3e81}.token.list{color:#ed6a43}.token.inserted{background-color:#eaffea;color:#55a532}.token.deleted{background-color:#ffecec;color:#bd2c00}.token.bold{font-weight:700}.token.italic{font-style:italic}.language-json .token.property{color:#183691}.language-markup .token.tag .token.punctuation{color:#333}.language-css .token.function,code.language-css{color:#0086b3}.language-yaml .token.atrule{color:#63a35c}code.language-yaml{color:#183691}.language-ruby .token.function{color:#333}.language-markdown .token.url{color:#795da3}.language-makefile .token.symbol{color:#795da3}.language-makefile .token.variable{color:#183691}.language-makefile .token.builtin{color:#0086b3}.language-bash .token.keyword{color:#0086b3}pre[data-line]{position:relative;padding:1em 0 1em 3em}pre[data-line] .line-highlight-wrapper{position:absolute;top:0;left:0;background-color:transparent;display:block;width:100%}pre[data-line] .line-highlight{position:absolute;left:0;right:0;padding:inherit 0;margin-top:1em;background:hsla(24,20%,50%,.08);background:linear-gradient(to right,hsla(24,20%,50%,.1) 70%,hsla(24,20%,50%,0));pointer-events:none;line-height:inherit;white-space:pre}pre[data-line] .line-highlight:before,pre[data-line] .line-highlight[data-end]:after{content:attr(data-start);position:absolute;top:.4em;left:.6em;min-width:1em;padding:0 .5em;background-color:hsla(24,20%,50%,.4);color:#f4f1ef;font:bold 65%/1.5 sans-serif;text-align:center;vertical-align:.3em;border-radius:999px;text-shadow:none;box-shadow:0 1px #fff}pre[data-line] .line-highlight[data-end]:after{content:attr(data-end);top:auto;bottom:.4em}html body{font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ol,html body>ul{margin-bottom:16px}html body ol,html body ul{padding-left:2em}html body ol.no-list,html body ul.no-list{padding:0;list-style-type:none}html body ol ol,html body ol ul,html body ul ol,html body ul ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:700;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::after,html body code::before{letter-spacing:-.2em;content:'\00a0'}html body pre>code{padding:0;margin:0;word-break:normal;white-space:pre;background:0 0;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:after,html body pre code:before,html body pre tt:after,html body pre tt:before{content:normal}html body blockquote,html body dl,html body ol,html body p,html body pre,html body ul{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body code,html body pre{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview ul{list-style:disc}.markdown-preview ul ul{list-style:circle}.markdown-preview ul ul ul{list-style:square}.markdown-preview ol{list-style:decimal}.markdown-preview ol ol,.markdown-preview ul ol{list-style-type:lower-roman}.markdown-preview ol ol ol,.markdown-preview ol ul ol,.markdown-preview ul ol ol,.markdown-preview ul ul ol{list-style-type:lower-alpha}.markdown-preview .newpage,.markdown-preview .pagebreak{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center!important}.markdown-preview:not([data-for=preview]) .code-chunk .code-chunk-btn-group{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .status{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .output-div{margin-bottom:16px}.markdown-preview .md-toc{padding:0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link div,.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}.markdown-preview .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0;min-height:100vh}@media screen and (min-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{font-size:14px!important;padding:1em}}@media print{html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc{padding:0 16px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link div,html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% - 300px);padding:2em calc(50% - 457px - 300px / 2);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */
.markdown-preview.markdown-preview {
  /* 样式化按钮 */
  /* 当鼠标悬停在按钮上时改变背景颜色 */
}
.markdown-preview.markdown-preview body {
  line-height: 150%;
  /* 设置行间距为当前字体大小的125% */
}
.markdown-preview.markdown-preview #toggleProofs {
  font: 0.8em;
  padding: 5px 5px;
  background-color: #4CAF50;
  color: white;
  border: none;
  border-radius: 5px;
  cursor: pointer;
  float: right;
  /* 将按钮浮动到右侧 */
}
.markdown-preview.markdown-preview #toggleProofs:hover {
  background-color: #45a049;
}
.markdown-preview.markdown-preview details {
  line-height: 200%;
  /* 设置行间距为当前字体大小的125% */
}
.markdown-preview.markdown-preview summary {
  font-weight: bold;
  font-style: italic;
}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

      </style>
      <!-- The content below will be included at the end of the <head> element. --><script type="text/javascript">
  document.addEventListener("DOMContentLoaded", function () {
    // your code here
  });
</script></head><body for="html-export">
    
    
      <div class="crossnote markdown-preview  ">
      
<div class="md-toc">
<details style="padding:0;;padding-left:0px;" open="">
        <summary class="md-toc-link-wrapper">
          <a href="#第二章-线性映射" class="md-toc-link"><p>第二章 线性映射</p>
</a>
          </summary>
        <div>
          <div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#212-definition-单射满射双射" class="md-toc-link">
            <p><em>2.1.2 Definition: 单射,满射,双射</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#214-definition-映射积" class="md-toc-link">
            <p><em>2.1.4 Definition: 映射积</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#215-definition-逆映射" class="md-toc-link">
            <p><em>2.1.5 Definition: 逆映射</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#221-definition-线性映射" class="md-toc-link">
            <p><em>2.2.1 Definition: 线性映射</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#225-example-正交投影变换" class="md-toc-link">
            <p><em>2.2.5 Example: 正交投影变换</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#221-corollary" class="md-toc-link">
            <p><em>2.2.1 Corollary</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#222-theorem-span-idthem222无关向量的映射span" class="md-toc-link">
            <p><em>2.2.2 Theorem: <span id="them2.2.2">无关向量的映射</span></em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#222-definition-线性映射的加法和数乘" class="md-toc-link">
            <p><em>2.2.2 Definition: 线性映射的加法和数乘</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#223-theorem-线性映射设空间" class="md-toc-link">
            <p><em>2.2.3 Theorem: 线性映射设空间</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#224-theorem-线性映射值空间和零空间" class="md-toc-link">
            <p><em>2.2.4 Theorem: 线性映射值空间和零空间</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#225-definition-亏秩定理" class="md-toc-link">
            <p><em>2.2.5 Definition: 亏秩定理</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#231-definition-矩阵" class="md-toc-link">
            <p><em>2.3.1 Definition: 矩阵</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#231-theorem-span-idthem231矩阵与线性映射关系span" class="md-toc-link">
            <p><em>2.3.1 Theorem: <span id="them2.3.1">矩阵与线性映射关系</span></em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#23-theorem-线性映射和矩阵的关系" class="md-toc-link">
            <p><em>2.3.* Theorem: 线性映射和矩阵的关系</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#232-definition-同构" class="md-toc-link">
            <p><em>2.3.2 Definition: 同构</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#232-theorem-线性映射和矩阵同构" class="md-toc-link">
            <p><em>2.3.2 Theorem: 线性映射和矩阵同构</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#233-theorem-线性空间同构" class="md-toc-link">
            <p><em>2.3.3 Theorem: 线性空间同构</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#231-corollary" class="md-toc-link">
            <p><em>2.3.1 Corollary</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#232-corollary" class="md-toc-link">
            <p><em>2.3.2 Corollary</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#233-corollary" class="md-toc-link">
            <p><em>2.3.3 Corollary</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#234-theorem-坐标变换" class="md-toc-link">
            <p><em>2.3.4 Theorem: 坐标变换</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#235-theorem-基变换对线性映射对应矩阵的影响" class="md-toc-link">
            <p><em>2.3.5 Theorem: 基变换对线性映射对应矩阵的影响</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#234-corollary-基变换对线性变换对应矩阵的影响" class="md-toc-link">
            <p><em>2.3.4 Corollary: 基变换对线性变换对应矩阵的影响</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#236-theorem-线性映射的维数性质" class="md-toc-link">
            <p><em>2.3.6 Theorem: 线性映射的维数性质</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#236-example" class="md-toc-link">
            <p><em>2.3.6 Example</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#241-theorem-线性变换特征值特征向量" class="md-toc-link">
            <p><em>2.4.1 Theorem: 线性变换特征值特征向量</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#241-example-特征向量空间" class="md-toc-link">
            <p><em>2.4.1 Example: 特征向量空间</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#242-definition-矩阵的特征值和特征向量" class="md-toc-link">
            <p><em>2.4.2 Definition: 矩阵的特征值和特征向量</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#24-theorem-线性变换特征值和矩阵特征值关系" class="md-toc-link">
            <p><em>2.4.* Theorem: 线性变换特征值和矩阵特征值关系</em></p>

          </a></div><details style="padding:0;;padding-left:24px;" open="">
        <summary class="md-toc-link-wrapper">
          <a href="#241-theorem-矩阵的迹与特征值" class="md-toc-link"><p><em>2.4.1 Theorem: 矩阵的迹与特征值</em></p>
</a>
          </summary>
        <div>
          <div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#lemma-维达定理" class="md-toc-link">
            <p>Lemma: 维达定理</p>

          </a></div>
        </div>
      </details>
    <div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#244-example-特征值的性质" class="md-toc-link">
            <p><em>2.4.4 Example: 特征值的性质</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#244-supplement-伴随矩阵" class="md-toc-link">
            <p><em>2.4.4 Supplement: 伴随矩阵</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#243-definition-特征子空间" class="md-toc-link">
            <p><em>2.4.3 Definition: 特征子空间</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#242-theorem-特征值几何重数定理" class="md-toc-link">
            <p><em>2.4.2 Theorem: 特征值几何重数定理</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#241-proposition-相似矩阵的性质" class="md-toc-link">
            <p><em>2.4.1 Proposition: 相似矩阵的性质</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#243-theorem-特征向量的线性无关性" class="md-toc-link">
            <p><em>2.4.3 Theorem: 特征向量的线性无关性</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#251-definition-正交变换和酉变换" class="md-toc-link">
            <p><em>2.5.1 Definition: 正交变换和酉变换</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#252-definition-正交矩阵和酉矩阵" class="md-toc-link">
            <p><em>2.5.2 Definition: 正交矩阵和酉矩阵</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#251-theorem-正交酉变换的性质" class="md-toc-link">
            <p><em>2.5.1 Theorem: 正交(酉)变换的性质</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#251-proposition-正交酉矩阵性质" class="md-toc-link">
            <p><em>2.5.1 Proposition: 正交（酉）矩阵性质</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#252-theorem-正交酉矩阵条件" class="md-toc-link">
            <p><em>2.5.2 Theorem: 正交（酉）矩阵条件</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#252-proposition-givens矩阵的性质" class="md-toc-link">
            <p><em>2.5.2 Proposition: Givens矩阵的性质</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#253-definition-householder矩阵" class="md-toc-link">
            <p><em>2.5.3 Definition: Householder矩阵</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#253-proposition-householder矩阵的性质" class="md-toc-link">
            <p><em>2.5.3 Proposition: Householder矩阵的性质</em></p>

          </a></div><div class="md-toc-link-wrapper" style="padding:0;;display:list-item;list-style:square;margin-left:42px">
          <a href="#253-example" class="md-toc-link">
            <p><em>2.5.3 Example</em></p>

          </a></div>
        </div>
      </details>
    
</div>
<br>
<button id="toggleProofs">Toggle Proofs</button><p></p>
<script>
  document.getElementById('toggleProofs').addEventListener('click', function() {
    var details = document.querySelectorAll('details > summary');
    for (var i = 0; i < details.length; i++) {
      if (details[i].textContent.includes('Proof')) {
        details[i].parentElement.open = !details[i].parentElement.open;
      }
    }
  });
</script>
<h4 id="第二章-线性映射">第二章 线性映射 </h4>
<h5 id="212-definition-单射满射双射"><em>2.1.2 Definition: 单射,满射,双射</em> </h5>
<blockquote>
<p>设<span class="mathjax-exps">$V$</span>和<span class="mathjax-exps">$W$</span>是两个非空集合，<span class="mathjax-exps">$f$</span>是<span class="mathjax-exps">$V$</span>到<span class="mathjax-exps">$W$</span>的一个映射。若对任意<span class="mathjax-exps">$x_1, x_2 \in V$</span>，当<span class="mathjax-exps">$x_1 \neq x_2$</span>时有<span class="mathjax-exps">$f(x_1) \neq f(x_2)$</span>，则称<span class="mathjax-exps">$f$</span>是<span class="mathjax-exps">$V$</span>到<span class="mathjax-exps">$W$</span>的单映射（简称单射）；若对任意<span class="mathjax-exps">$y \in W$</span>都有一个元素<span class="mathjax-exps">$x \in V$</span>使得<span class="mathjax-exps">$f(x) = y$</span>（即<span class="mathjax-exps">$R(f) = W$</span>），则称<span class="mathjax-exps">$f$</span>是<span class="mathjax-exps">$V$</span>到<span class="mathjax-exps">$W$</span>的满映射（简称满射）；若映射<span class="mathjax-exps">$f$</span>既是单映射又是满映射，则称<span class="mathjax-exps">$f$</span>是<span class="mathjax-exps">$V$</span>到<span class="mathjax-exps">$W$</span>的一一映射或双映射（简称双射）。</p>
</blockquote>
<h5 id="214-definition-映射积"><em>2.1.4 Definition: 映射积</em> </h5>
<blockquote>
<p>设<span class="mathjax-exps">$V_1, V_2$</span>和<span class="mathjax-exps">$V_3$</span>是三个非空集合，并设<span class="mathjax-exps">$f_1$</span>是<span class="mathjax-exps">$V_1$</span>到<span class="mathjax-exps">$V_2$</span>的一个映射，<span class="mathjax-exps">$f_2$</span>是<span class="mathjax-exps">$V_2$</span>到<span class="mathjax-exps">$V_3$</span>的一个映射。由<span class="mathjax-exps">$f_1$</span>和<span class="mathjax-exps">$f_2$</span>确定的<span class="mathjax-exps">$V_1$</span>到<span class="mathjax-exps">$V_3$</span>的映射<span class="mathjax-exps">$f: x \rightarrow f(f_1(x)), x \in V_1$</span>，称为映射<span class="mathjax-exps">$f_1$</span>和<span class="mathjax-exps">$f_2$</span>的乘积，记为<span class="mathjax-exps">$f_3 = f_2 \cdot f_1$</span>，或简写为<span class="mathjax-exps">$f = f_2 f_1$</span>。</p>
</blockquote>
<h5 id="215-definition-逆映射"><em>2.1.5 Definition: 逆映射</em> </h5>
<blockquote>
<p>设有映射<span class="mathjax-exps">$f_1: V \rightarrow W$</span>，若存在映射<span class="mathjax-exps">$f_2: W \rightarrow V$</span>使得<span class="mathjax-exps">$f_2 \cdot f_1 = I_V$</span>和<span class="mathjax-exps">$f_1 \cdot f_2 = I_W$</span>，其中<span class="mathjax-exps">$I_V: x \rightarrow x, x \in V$</span>为<span class="mathjax-exps">$V$</span>上的恒等映射，<span class="mathjax-exps">$I_W$</span>是<span class="mathjax-exps">$W$</span>上的恒等映射。我们称<span class="mathjax-exps">$f_2$</span>为<span class="mathjax-exps">$f_1$</span>的逆映射，记为<span class="mathjax-exps">$f_1'$</span>。若映射<span class="mathjax-exps">$f_1$</span>有逆映射，则称<span class="mathjax-exps">$f_1$</span>为可逆映射。</p>
</blockquote>
<h5 id="221-definition-线性映射"><em>2.2.1 Definition: 线性映射</em> </h5>
<blockquote>
<p>设 <span class="mathjax-exps">$V$</span> 和 <span class="mathjax-exps">$W$</span> 是数域 <span class="mathjax-exps">$F$</span> 上的线性空间，如果映射 <span class="mathjax-exps">$T: V \to W$</span> 满足下述性质：<br>
(1) 可加性：对任意 <span class="mathjax-exps">$x, y \in V, T(x + y) = T(x) + T(y)$</span>;<br>
(2) 齐次性：对任意 <span class="mathjax-exps">$x \in V, \lambda \in F, T(\lambda x) = \lambda T(x)$</span>;<br>
则称 <span class="mathjax-exps">$T$</span> 为 <span class="mathjax-exps">$V$</span> 到 <span class="mathjax-exps">$W$</span> 上的线性映射。特别地，当 <span class="mathjax-exps">$V = W$</span> 时，映射 <span class="mathjax-exps">$T: V \to V$</span> 称为 <span class="mathjax-exps">$V$</span> 上的线性变换（或线性算子）。</p>
</blockquote>
<h5 id="225-example-正交投影变换"><em>2.2.5 Example: 正交投影变换</em> </h5>
<blockquote>
<p>设向量组 <span class="mathjax-exps">$\alpha_1,\cdots,\alpha_p$</span> 是 <span class="mathjax-exps">$W$</span> 的一组标准正交基，则 <span class="mathjax-exps">$V$</span> 中任一向量 <span class="mathjax-exps">$x$</span> 在 <span class="mathjax-exps">$W$</span> 上的正交投影为<br>
</p><div class="mathjax-exps">\[\text{ Proj}_{w} x=\left(x,\alpha_1\right)\alpha_1+\cdots+\left(x,\alpha_p\right)\alpha_p\]</div>因此，<br>
<div class="mathjax-exps">\[T(x)=\sum_{i=1}^p\left(x,\alpha_i\right)\alpha_i\]</div>那么，对任意向量 <span class="mathjax-exps">$x, y\in V$</span> 和 <span class="mathjax-exps">$\lambda,\mu\in F$</span> 有<br>
<div class="mathjax-exps">\[\begin{align*} T(\lambda x+\mu y)&amp;=\sum_{i=1}^p\left(\lambda x+\mu y,\alpha_i\right)\alpha_i\\ &amp;=\lambda\sum_{i=1}^p\left(x,\alpha_i\right)\alpha_i+\mu\sum_{i=1}^p\left(y,\alpha_i\right)\alpha_i\\ &amp;=\lambda T(x)+\mu T(y)\end{align*}\]</div> 综上可知，映射 <span class="mathjax-exps">$T$</span> 是 <span class="mathjax-exps">$V$</span> 上的线性变换。<p></p>
</blockquote>
<h5 id="221-corollary"><em>2.2.1 Corollary</em> </h5>
<blockquote>
<p>设 <span class="mathjax-exps">$V$</span> 和 <span class="mathjax-exps">$W$</span> 是数域 <span class="mathjax-exps">$F$</span> 上的线性空间，<span class="mathjax-exps">$T: V \to W$</span> 是线性映射, 若<span class="mathjax-exps">$\alpha  _1, \cdots, \alpha  _n$</span><br>
(1) <span class="mathjax-exps">$T(\sum_{i=1}^{n} \lambda_i \alpha_i) = \sum_{i=1}^{n} \lambda_i T(\alpha_i)$</span>;<br>
(2) <span class="mathjax-exps">$T(0) = 0$</span>;<br>
(3) <span class="mathjax-exps">$T(-\alpha) = -T(\alpha)$</span>, <span class="mathjax-exps">$\forall \alpha \in V$</span>;<br>
(4) 若 <span class="mathjax-exps">$\alpha  _1, \cdots, \alpha  _n$</span> 线性相关，则 <span class="mathjax-exps">$T(\alpha  _1), \cdots, T(\alpha  _n )$</span> 也线性相关;<br>
(5) 若<span class="mathjax-exps">$T(\alpha  _1), \cdots, T(\alpha  _n )$</span>  线性无关，则  <span class="mathjax-exps">$\alpha  _1, \cdots, \alpha  _n$</span>也线性无关。</p>
</blockquote>
<details open="">
  <summary>Proof:</summary>
<p>(1) 由线性映射的定义显然可得.<br>
(2) <span class="mathjax-exps">$T(0) = T(0\cdot 0) = 0\cdot T(0) = 0$</span>.<br>
(3) <span class="mathjax-exps">$T(-\alpha) = T(-1\cdot \alpha) = -1\cdot T(\alpha) = -T(\alpha)$</span>.<br>
(4) 由 (5)显然可得.<br>
(5) 若<span class="mathjax-exps">$k_1T(\alpha  _1),+ \cdots,+ k_nT(\alpha  _n ) = 0$</span> 只在<span class="mathjax-exps">$k_1 = \cdots = k_n = 0$</span>时成立, 则<span class="mathjax-exps">$T(k_1 \alpha_1+ \cdots+ k_n\alpha  _n)=0$</span> 也只在<span class="mathjax-exps">$k_1 = \cdots = k_n = 0$</span>成立, 若 <span class="mathjax-exps">$\alpha  _1, \cdots, \alpha  _n$</span> 线性相关,  则存在不全为0的<span class="mathjax-exps">$k_1, \cdots, k_n$</span>使得<span class="mathjax-exps">$k_1\alpha  _1 + \cdots + k_n\alpha  _n = 0$</span>, 则 <span class="mathjax-exps">$T(k_1\alpha  _1 + \cdots + k_n\alpha  _n) = 0$</span> 且不全为0, 与假设矛盾, 故 <span class="mathjax-exps">$\alpha  _1, \cdots, \alpha  _n$</span> 线性无关. 反之亦然.</p>
</details>
<h5 id="222-theorem-span-idthem222无关向量的映射span"><em>2.2.2 Theorem: <span id="them2.2.2">无关向量的映射</span></em> </h5>
<blockquote>
<p>设 <span class="mathjax-exps">$T: V \to W$</span> 是线性映射, 当且仅当<span class="mathjax-exps">$T$</span>是单射时，<span class="mathjax-exps">$T$</span>将线性无关的向量映为线性无关的向量.<br>
{注}: <span class="mathjax-exps">$\dim {V} \le \dim {W}$</span></p>
</blockquote>
<details open="">
  <summary>Proof:</summary> 
<p>充分性。设 <span class="mathjax-exps">$\alpha_1,\cdots,\alpha_p$</span> 是 <span class="mathjax-exps">$V$</span> 中一组线性无关向量，则对任一不全为零的数组 <span class="mathjax-exps">$k_1,\cdots, k_p\in F$</span>，有 <span class="mathjax-exps">$k_1\alpha_1+\cdots+k_p\alpha_p\neq\theta$</span>。由于 <span class="mathjax-exps">$T$</span> 是单射且 <span class="mathjax-exps">$T(\theta)=\theta$</span>，从而<br>
</p><div class="mathjax-exps">\[T\left(k_1\alpha_1+\cdots+k_p\alpha_p\right)=k_1 T\left(\alpha_1\right)+\cdots+k_p T\left(\alpha_p\right)\neq\theta\]</div> 上式说明 <span class="mathjax-exps">$T\left(\alpha_1\right),\cdots, T\left(\alpha_p\right)$</span> 是 <span class="mathjax-exps">$W$</span> 的一组线性无关向量。<br>
必要性。设 <span class="mathjax-exps">$\zeta_1,\cdots,\zeta_n$</span> 是 <span class="mathjax-exps">$V$</span> 中一组基，则 <span class="mathjax-exps">$V$</span> 中任意向量 <span class="mathjax-exps">$\alpha$</span> 和 <span class="mathjax-exps">$\beta$</span> 可表示为<br>
<div class="mathjax-exps">\[\alpha=a_1\zeta_1+\cdots+a_n\zeta_n,\quad\beta=b_1\zeta_1+\cdots+b_n\zeta_n\]</div> 式中，<span class="mathjax-exps">$a=\left[a_1,\cdots, a_n\right]^T$</span> 和 <span class="mathjax-exps">$b=\left[b_1,\cdots, b_n\right]^T$</span> 分别是向量 <span class="mathjax-exps">$\alpha$</span> 和 <span class="mathjax-exps">$\beta$</span> 在基 <span class="mathjax-exps">$\zeta_1,\cdots,\zeta_n$</span> 下的坐标。于是<br>
<div class="mathjax-exps">\[\begin{gathered} T(\alpha)=a_1 T\left(\zeta_1\right)+\cdots+a_n T\left(\zeta_n\right)\\ T(\beta)=b_1 T\left(\zeta_1\right)+\cdots+b_n T\left(\zeta_n\right)\end{gathered}\]</div> 且有<br>
<div class="mathjax-exps">\[T(\alpha)-T(\beta)=\left(a_1-b_1\right) T\left(\zeta_1\right)+\cdots+\left(a_n-b_n\right) T\left(\zeta_n\right)\]</div> 当 <span class="mathjax-exps">$\alpha\neq\beta$</span> 时，若有 <span class="mathjax-exps">$T(\alpha)=T(\beta)$</span>，则根据上式得<br>
<div class="mathjax-exps">\[\left(a_1-b_1\right) T\left(\zeta_1\right)+\cdots+\left(a_n-b_n\right) T\left(\zeta_n\right)=\theta\]</div> 由于 <span class="mathjax-exps">$T\left(\zeta_1\right),\cdots, T\left(\zeta_n\right)$</span> 线性无关，故 <span class="mathjax-exps">$a_i=b_i, i=1,\cdots, n$</span>，即 <span class="mathjax-exps">$a=b$</span>。由于 <span class="mathjax-exps">$\alpha\neq\beta$</span>，故其坐标 <span class="mathjax-exps">$a\neq b$</span>。因此，当 <span class="mathjax-exps">$\alpha\neq\beta$</span> 时，<span class="mathjax-exps">$T(\alpha)\neq T(\beta)$</span>。这表明映射 <span class="mathjax-exps">$T$</span> 是单射。证毕。<p></p>
</details>
<h5 id="222-definition-线性映射的加法和数乘"><em>2.2.2 Definition: 线性映射的加法和数乘</em> </h5>
<blockquote>
<p>设 <span class="mathjax-exps">$V$</span> 和 <span class="mathjax-exps">$W$</span> 是数域 <span class="mathjax-exps">$F$</span> 上的线性空间，<span class="mathjax-exps">$T: V \to W$</span> 是线性映射，定义 <span class="mathjax-exps">$T$</span> 的加法和数乘如下：<br>
(1) <span class="mathjax-exps">$T_1 + T_2: V \to W, (T_1 + T_2)(x) = T_1(x) + T_2(x)$</span>;<br>
(2) <span class="mathjax-exps">$\lambda T: V \to W, (\lambda T)(x) = \lambda T(x)$</span>。<br>
则 <span class="mathjax-exps">$T_1 + T_2$</span> 和 <span class="mathjax-exps">$\lambda T$</span> 仍然是 <span class="mathjax-exps">$V$</span> 到 <span class="mathjax-exps">$W$</span> 上的线性映射。</p>
</blockquote>
<p>{注}: 线性空间<span class="mathjax-exps">$\mathcal{L}(V,W)$</span>的维数为<span class="mathjax-exps">$\dim \mathcal{L}(V,W) = \dim V \cdot \dim W$</span>。</p>
<h5 id="223-theorem-线性映射设空间"><em>2.2.3 Theorem: 线性映射设空间</em> </h5>
<blockquote>
<p>集合<span class="mathjax-exps">$\mathcal{L}(V, W)$</span>对定义2.2.2的加法和定义2.2.3的数乘构成数域<span class="mathjax-exps">$F$</span>上的线性空间，称为线性映射空间。特别地，<span class="mathjax-exps">$\mathcal{L}(V)$</span>称为线性变换空间。</p>
</blockquote>
<h5 id="224-theorem-线性映射值空间和零空间"><em>2.2.4 Theorem: 线性映射值空间和零空间</em> </h5>
<blockquote>
<p>设<span class="mathjax-exps">$T \in \mathcal{L}(V, W)$</span>，定义<br>
</p><div class="mathjax-exps">$$\begin{array}{ll} N(T) = \{x \in V \mid T(x) = \theta\}\\ R(T) = \{y \in W \mid y = T(x), \forall x \in V\} \end{array}$$</div> 则<span class="mathjax-exps">$N(T)$</span>是<span class="mathjax-exps">$V$</span>的子空间，<span class="mathjax-exps">$R(T)$</span>是<span class="mathjax-exps">$W$</span>的子空间。我们称<span class="mathjax-exps">$N(T)$</span>是线性映射<span class="mathjax-exps">$T$</span>的核空间（或零空间），<span class="mathjax-exps">$R(T)$</span>是线性映射<span class="mathjax-exps">$T$</span>的像空间（或值空间）；并称<span class="mathjax-exps">$\operatorname{dim} N(T)$</span>为线性映射<span class="mathjax-exps">$T$</span>的零度（或亏），<span class="mathjax-exps">$\operatorname{dim} R(T)$</span>为线性映射<span class="mathjax-exps">$T$</span>的秩。<p></p>
</blockquote>
<h5 id="225-definition-亏秩定理"><em>2.2.5 Definition: 亏秩定理</em> </h5>
<blockquote>
<p>设 <span class="mathjax-exps">$T \in \mathcal{L}(V,W)$</span>, 则 <span class="mathjax-exps">$\dim N(T) + \dim R(T) = \dim V$</span>.</p>
</blockquote>
<details open="">
  <summary>Proof:</summary> 
<p>设线性空间 <span class="mathjax-exps">$V$</span> 的维数为 <span class="mathjax-exps">$n$</span>，其子空间 <span class="mathjax-exps">$N(T)$</span> 的维数为 <span class="mathjax-exps">$m$</span>。在 <span class="mathjax-exps">$N(T)$</span> 中取一组基 <span class="mathjax-exps">$\alpha_1,\cdots,\alpha_m$</span>，并把它扩充为 <span class="mathjax-exps">$V$</span> 的基 <span class="mathjax-exps">$\alpha_1,\cdots,\alpha_m,\alpha_{m+1},\cdots,\alpha_n$</span>，则对任意向量 <span class="mathjax-exps">$x\in V$</span> 有<br>
</p><div class="mathjax-exps">\[x=\sum_{i=1}^n k_i\alpha_i,\quad k_i\in F\]</div> 从而<br>
<div class="mathjax-exps">\[T x=\sum_{i=1}^n k_i T\left(\alpha_i\right)\]</div> 即<br>
<div class="mathjax-exps">\[R(T)=\operatorname{span}\left(T\left(\alpha_1\right),\cdots, T\left(\alpha_m\right), T\left(\alpha_{m+1}\right),\cdots, T\left(\alpha_n\right)\right)\]</div> 注意到 <span class="mathjax-exps">$\alpha_1,\cdots,\alpha_m\in N(T)$</span>，故 <span class="mathjax-exps">$T\left(\alpha_1\right)=\cdots=T\left(\alpha_m\right)=\theta$</span>。由此，式可改写为<br>
<div class="mathjax-exps">\[R(T)=\operatorname{span}\left(T\left(\alpha_{m+1}\right),\cdots, T\left(\alpha_n\right)\right)\]</div> 现只需证明 <span class="mathjax-exps">$T\left(\alpha_{m+1}\right),\cdots, T\left(\alpha_n\right)$</span> 线性无关，即可说明 <span class="mathjax-exps">$T\left(\alpha_{m+1}\right),\cdots, T\left(\alpha_n\right)$</span> 是 <span class="mathjax-exps">$R(T)$</span> 的一组基。<br>
假设存在一组数 <span class="mathjax-exps">$l_i\in F, i=m+1,\cdots, n$</span>，使得<br>
<div class="mathjax-exps">\[\sum_{i=m+1}^n l_i T\left(\alpha_i\right)=\theta\]</div> 则根据线性映射定义知 <div class="mathjax-exps">\[T\left(\sum_{i=m+1}^n l_i\alpha_i\right)=\theta\]</div> 即 <div class="mathjax-exps">\[\sum_{i=m+1}^n l_i\alpha_i\in N(T)\]</div> 又知向量组 <span class="mathjax-exps">$\alpha_1,\cdots,\alpha_m$</span> 是 <span class="mathjax-exps">$N(T)$</span> 的一组基，故向量 <span class="mathjax-exps">$\sum_{i=m+1}^n l_i\alpha_i$</span> 一定可由向量组 <span class="mathjax-exps">$\alpha_1,\cdots,\alpha_m$</span> 线性表示，即<br>
<div class="mathjax-exps">\[\sum_{i=m+1}^n l_i\alpha_i=\sum_{i=1}^m l_i\alpha_i\]</div> 整理得<br>
<div class="mathjax-exps">$$\sum_{i=1}^{n} l_i \alpha_i = 0$$</div> 已知向量组 <span class="mathjax-exps">$\alpha_1,\cdots,\alpha_m,\alpha_{m+1},\cdots,\alpha_n$</span> 线性无关，故上式成立的充分必要条件为 <span class="mathjax-exps">$l_i=0, i=1,\cdots, n$</span>。因此，我们推断出 <span class="mathjax-exps">$T\left(\alpha_{m+1}\right),\cdots, T\left(\alpha_n\right)$</span> 线性无关，即 <span class="mathjax-exps">$\text{dim } R(T) = n - m$</span>。证毕。<p></p>
</details>
<h5 id="231-definition-矩阵"><em>2.3.1 Definition: 矩阵</em> </h5>
<blockquote>
<p>设<span class="mathjax-exps">$T$</span>是<span class="mathjax-exps">$V$</span>到<span class="mathjax-exps">$W$</span>上的线性映射, <span class="mathjax-exps">$\epsilon _1, \cdots, \epsilon _n$</span> 和 <span class="mathjax-exps">$\eta _1, \cdots, \eta _m$</span> 分别是 <span class="mathjax-exps">$V$</span> 和 <span class="mathjax-exps">$W$</span> 的基.<br>
设<span class="mathjax-exps">$T(\epsilon  _1, \cdots, \epsilon  _n) = [T(\epsilon _1), \cdots , T(\epsilon _n)] = [\eta _1, \cdots, \eta _n  ]A$</span>, <span class="mathjax-exps">$A \in F^{m \times n}$</span>, 则称<span class="mathjax-exps">$A$</span>是<span class="mathjax-exps">$T$</span>在基<span class="mathjax-exps">$\epsilon _1, \cdots, \epsilon _n$</span>和<span class="mathjax-exps">$\eta _1, \cdots, \eta _m$</span>下的矩阵.</p>
</blockquote>
<p>{注}:</p>
<ol>
<li>基不一定为列向量,可以为矩阵或多项式</li>
<li><span class="mathjax-exps">$A$</span>与<span class="mathjax-exps">$T$</span>一一对应(在基确定的情况下)</li>
<li>当<span class="mathjax-exps">$V = W$</span>时, <span class="mathjax-exps">$A$</span>称为线性变换<span class="mathjax-exps">$T$</span>的矩阵</li>
<li>矩阵是线性映射在某组基下的表达(或坐标的线性变换)</li>
</ol>
<h5 id="231-theorem-span-idthem231矩阵与线性映射关系span"><em>2.3.1 Theorem: <span id="them2.3.1">矩阵与线性映射关系</span></em> </h5>
<blockquote>
<p>设<span class="mathjax-exps">$V$</span>和<span class="mathjax-exps">$W$</span>是数域<span class="mathjax-exps">$F$</span>上的线性空间，取定<span class="mathjax-exps">$\varepsilon_1, \cdots, \varepsilon_n$</span>和<span class="mathjax-exps">$\eta_1, \cdots, \eta_m$</span>分别是<span class="mathjax-exps">$V$</span>和<span class="mathjax-exps">$W$</span>的一组基。任取<span class="mathjax-exps">$A = (a_{ij}) \in F^{m \times n}$</span>，则有且仅有一个线性映射<span class="mathjax-exps">$T \in \mathcal{L}(V, W)$</span>使其在<span class="mathjax-exps">$V$</span>的基<span class="mathjax-exps">$\varepsilon_1, \cdots, \varepsilon_n$</span>和<span class="mathjax-exps">$W$</span>的基<span class="mathjax-exps">$\eta_1, \cdots, \eta_m$</span>下的矩阵恰为<span class="mathjax-exps">$A$</span>。</p>
</blockquote>
<details open="">
  <summary>Proof:</summary> 
<p>先证明存在性.<br>
设 <span class="mathjax-exps">$\forall x, y \in V$</span>, 其可表示为 <span class="mathjax-exps">$x = \sum_{j=1}^n \alpha_j \varepsilon_j, y = \sum_{j=1}^n \beta_j \varepsilon_j$</span>, 则定义映射<span class="mathjax-exps">$T: V \rightarrow W$</span>使得满足下列关系:<br>
</p><div class="mathjax-exps">\[T(x) = T(\sum_{j=1}^n \alpha_j \varepsilon_j)= \sum_{j=1}^{n} \sum_{i=1}^{m} \alpha_{j} a_{ij} \eta_{i}\]</div> <span class="mathjax-exps">$\forall x, y \in 和 \lambda, \mu \in F W$</span> 有 <div class="mathjax-exps">\[\begin{align*} T(\lambda x + \mu y) &amp;= \sum_{j=1}^{n} \sum_{i=1}^{m} \left( \lambda \alpha_{j} + \mu \beta_{j} \right) a_{ij} \eta_{i} \\ &amp;= \lambda \sum_{j=1}^{n} \sum_{i=1}^{m} \alpha_{j} a_{ij} \eta_{i} + \mu \sum_{j=1}^{n} \sum_{i=1}^{m} \beta_{j} a_{ij} \eta_{i} \\ &amp;= \lambda T(x) + \mu T(y) \end{align*}\]</div><br>
由映射 <span class="mathjax-exps">\(T\)</span> 定义式知，<span class="mathjax-exps">\(\forall k = 1, \cdots, n\)</span>，则 <div class="mathjax-exps">\[T(\varepsilon_{k}) = \sum_{i=1}^{m} a_{ik} \eta_{i} = \left[\eta_{1}, \cdots, \eta_{m}\right] \left[\begin{array}{l} a_{1k} \\ \vdots \\ a_{mk} \end{array}\right]\]</div> 则有<div class="mathjax-exps">\[\left[T(\varepsilon_{1}), \cdots, T(\varepsilon_{n})\right] = \left[\eta_{1}, \cdots, \eta_{m}\right] A\]</div>显然可以看出映射 <span class="mathjax-exps">\(T\)</span> 在 <span class="mathjax-exps">\(V\)</span> 的基 <span class="mathjax-exps">\(\varepsilon_1, \cdots, \varepsilon_n\)</span> 和 <span class="mathjax-exps">\(W\)</span> 的基 <span class="mathjax-exps">\(\eta_1, \cdots, \eta_m\)</span> 下的矩阵为 <span class="mathjax-exps">\(A\)</span>.<br>
下面证明唯一性.<br>
假设存在另一个线性映射 <span class="mathjax-exps">\(\tilde{T}\)</span>，且它在 <span class="mathjax-exps">\(V\)</span> 的基 <span class="mathjax-exps">\(\varepsilon_1, \cdots, \varepsilon_n\)</span> 和 <span class="mathjax-exps">\(W\)</span> 的基 <span class="mathjax-exps">\(\eta_1, \cdots, \eta_m\)</span> 下的矩阵也为 <span class="mathjax-exps">\(A\)</span>。根据矩阵 <span class="mathjax-exps">\(A\)</span> 的定义，有 <div class="mathjax-exps">\[\begin{align*} &amp;\tilde{T}(\varepsilon_k) = \left[\eta_1, \cdots, \eta_m\right] \left[\begin{array}{l} a_{1k} \\ \vdots \\ a_{mk} \end{array}\right] \quad \forall k = 1, \cdots, n \end{align*}\]</div> 即 <span class="mathjax-exps">\(T(\varepsilon_k) = \tilde{T}(\varepsilon_k), k = 1, \cdots, n\)</span>。因此，<span class="mathjax-exps">\(T = \tilde{T}\)</span>，即线性映射 <span class="mathjax-exps">\(T\)</span> 是唯一的。证毕。<p></p>
</details>
<p>{注}:</p>
<ol>
<li>给定一组基的情况下, 线性映射与矩阵一一对应</li>
<li>类似于向量与坐标之间的关系</li>
<li>在同一个变换下, 矩阵可以描述基变换</li>
</ol>
<h5 id="23-theorem-线性映射和矩阵的关系"><em>2.3.* Theorem: 线性映射和矩阵的关系</em> </h5>
<blockquote>
<p>定理2.3.1表明线性映射<span class="mathjax-exps">$T \in \mathcal{L}(V, W)$</span>和<span class="mathjax-exps">$A = (a_{ij}) \in F^{m \times n}$</span>存在着一一对应的关系，即存在着双射<span class="mathjax-exps">$f: \mathcal{L}(V, W) \rightarrow F^{m \times n}$</span>满足<span class="mathjax-exps">$f(T) = A$</span>。</p>
</blockquote>
<h5 id="232-definition-同构"><em>2.3.2 Definition: 同构</em> </h5>
<blockquote>
<p>设<span class="mathjax-exps">$V$</span>和<span class="mathjax-exps">$W$</span>是数域<span class="mathjax-exps">$F$</span>上的线性空间，若存在双射<span class="mathjax-exps">$f: V \rightarrow W$</span>满足<br>
<span class="mathjax-exps">$(1) f(x + y) = f(x) + f(y);$</span><br>
<span class="mathjax-exps">$(2) f(\lambda x) = \lambda f(x).$</span><br>
其中：<span class="mathjax-exps">$x$</span>和<span class="mathjax-exps">$y$</span>是<span class="mathjax-exps">$V$</span>中任意向量，<span class="mathjax-exps">$\lambda$</span>是数域<span class="mathjax-exps">$F$</span>的任意数，则称<span class="mathjax-exps">$f$</span>是<span class="mathjax-exps">$V$</span>到<span class="mathjax-exps">$W$</span>的同构映射，并称线性空间<span class="mathjax-exps">$V$</span>与<span class="mathjax-exps">$W$</span>同构。</p>
</blockquote>
<h5 id="232-theorem-线性映射和矩阵同构"><em>2.3.2 Theorem: 线性映射和矩阵同构</em> </h5>
<blockquote>
<p>设<span class="mathjax-exps">$V$</span>和<span class="mathjax-exps">$W$</span>是数域<span class="mathjax-exps">$F$</span>上的线性空间，它们的维数分别为<span class="mathjax-exps">$n$</span>和<span class="mathjax-exps">$m$</span>，则线性映射空间<span class="mathjax-exps">$\mathcal{L}(V, W)$</span>和矩阵空间<span class="mathjax-exps">$F^{m \times n}$</span>同构。</p>
</blockquote>
<details open="">
  <summary>Proof:</summary> 
<p>由 <a href="#them2.3.1">定理2.3.1</a>可以看出线性映射 <span class="mathjax-exps">$T \in \mathcal{L}(V,W)$</span>和矩阵<span class="mathjax-exps">$A \in F^{m \times n}$</span> 存在一一对应的关系, 即存在着双射 <span class="mathjax-exps">\(f: \mathcal{L}(V, W) \rightarrow F^{m \times n}\)</span> 满足 </p><div class="mathjax-exps">\[f(T) = A\]</div> 实际上，映射 <span class="mathjax-exps">\(f\)</span> 不仅是双射，还是线性映射，具体原因如下：<br>
设 <span class="mathjax-exps">\(T_1, T_2 \in \mathcal{L}(V, W)\)</span>，并令 <span class="mathjax-exps">\(T_1\)</span> 和 <span class="mathjax-exps">\(T_2\)</span> 在 <span class="mathjax-exps">\(V\)</span> 的基 <span class="mathjax-exps">\(\varepsilon_1, \cdots, \varepsilon_n\)</span> 和 <span class="mathjax-exps">\(W\)</span> 的基 <span class="mathjax-exps">\(\eta_1, \cdots, \eta_m\)</span> 下的矩阵分别为 <span class="mathjax-exps">\(A_1\)</span> 和 <span class="mathjax-exps">\(A_2\)</span>，则对任意 <span class="mathjax-exps">\(\lambda, \mu \in F\)</span>，有 <span class="mathjax-exps">\(\lambda T_1 + \mu T_2\)</span> 是线性映射，且<br>
<div class="mathjax-exps">\[\begin{align*} (\lambda T_1 + \mu T_2)(\varepsilon_1, \cdots, \varepsilon_n) &amp;= [(\lambda T_1 + \mu T_2)(\varepsilon_1), \cdots, (\lambda T_1 + \mu T_2)(\varepsilon_n)] \\ &amp;= [\lambda T_1(\varepsilon_1) + \mu T_2(\varepsilon_1), \cdots, \lambda T_1(\varepsilon_n) + \mu T_2(\varepsilon_n)] \\ &amp;= \lambda [T_1(\varepsilon_1), \cdots, T_1(\varepsilon_n)] + \mu [T_2(\varepsilon_1), \cdots, T_2(\varepsilon_n)] \\ &amp;= \lambda [\eta_1, \cdots, \eta_m] A_1 + \mu [\eta_1, \cdots, \eta_m] A_2 \\ &amp;= [\eta_1, \cdots, \eta_m] (\lambda A_1 + \mu A_2) \end{align*}\]</div> 上式表明线性映射 <span class="mathjax-exps">\(\lambda T_1 + \mu T_2\)</span> 在 <span class="mathjax-exps">\(V\)</span> 的基 <span class="mathjax-exps">\(\varepsilon_1, \cdots, \varepsilon_n\)</span> 和 <span class="mathjax-exps">\(W\)</span> 的基 <span class="mathjax-exps">\(\eta_1, \cdots, \eta_m\)</span> 下的矩阵为 <span class="mathjax-exps">\(\lambda A_1 + \mu A_2\)</span>，即 <div class="mathjax-exps">\[f(\lambda T_1 + \mu T_2) = \lambda A_1 + \mu A_2\]</div><p></p>
</details>
<p><em>2.3 Proposition: 同构映射的性质</em></p>
<blockquote>
<p>设 <span class="mathjax-exps">$V$</span> 和 <span class="mathjax-exps">$W$</span> 是数域 <span class="mathjax-exps">$F$</span> 上的线性空间，<span class="mathjax-exps">$T: V \to W$</span> 是同构映射，则有</p>
<ol>
<li><span class="mathjax-exps">$T(0) = 0', 0 \in V, 0' \in W$</span></li>
<li><span class="mathjax-exps">$T(-x) = -T(x)$</span>, 对于所有 <span class="mathjax-exps">$x \in V$</span></li>
<li><span class="mathjax-exps">$T\left(\sum \alpha_i x_i\right) = \sum \alpha_i T(x_i)$</span>, 对于所有 <span class="mathjax-exps">$\alpha_i \in F$</span> 和 <span class="mathjax-exps">$x_i \in V$</span></li>
<li>V中的向量组 <span class="mathjax-exps">$x_1, \cdots, x_r$</span> 线性相关，当且仅当其像 <span class="mathjax-exps">$T(x_1), \cdots, T(x_r)$</span> 线性相关</li>
<li>若 <span class="mathjax-exps">$\varepsilon_1, \cdots, \varepsilon_n$</span> 是 V 的一组基，则 <span class="mathjax-exps">$T(\varepsilon_1), \cdots, T(\varepsilon_n)$</span> 是 W 的一组基</li>
<li>T的逆映射 <span class="mathjax-exps">$T^{-1}: W \rightarrow V$</span> 存在且是同构映射</li>
</ol>
</blockquote>
<details open="">
  <summary>Proof:</summary> 
<p>1~4显然, 下面证5.<br>
由<a href="#them2.2.2">定理2.2.2</a>知，若 <span class="mathjax-exps">\(\varepsilon_1, \cdots, \varepsilon_n\)</span> 是 <span class="mathjax-exps">\(V\)</span> 的一组基，则向量组 <span class="mathjax-exps">\(T(\varepsilon_1), \cdots, T(\varepsilon_n)\)</span> 必线性无关。又知对任意向量 <span class="mathjax-exps">\(y \in W\)</span>，必存在 <span class="mathjax-exps">\(x \in V\)</span> 使得 <span class="mathjax-exps">\(T(x) = y\)</span>，其中 <span class="mathjax-exps">\(x\)</span> 可由基 <span class="mathjax-exps">\(\varepsilon_1, \cdots, \varepsilon_n\)</span> 线性表示为 <span class="mathjax-exps">\(x = \sum_{j=1}^n \alpha_j \varepsilon_j\)</span>。由此</p><div class="mathjax-exps">\[y = T\left(\sum_{j=1}^n \alpha_j \varepsilon_j\right) = \sum_{j=1}^n \alpha_j T(\varepsilon_j)\]</div> 即 <span class="mathjax-exps">\(W\)</span> 中任意向量 <span class="mathjax-exps">\(y\)</span> 总可由向量组 <span class="mathjax-exps">\(T(\varepsilon_1), \cdots, T(\varepsilon_n)\)</span> 线性表示。于是， <span class="mathjax-exps">\(T(\varepsilon_1), \cdots, T(\varepsilon_n)\)</span> 是 <span class="mathjax-exps">\(W\)</span> 的一组基。这同时表明线性空间 <span class="mathjax-exps">\(W\)</span> 的维数与线性空间 <span class="mathjax-exps">\(V\)</span> 的维数相同。<p></p>
</details>
<h5 id="233-theorem-线性空间同构"><em>2.3.3 Theorem: 线性空间同构</em> </h5>
<blockquote>
<p>线性空间同构当仅当它们的维数相等。</p>
</blockquote>
<details open="">
  <summary>Proof:</summary> 
<p>必要性是显然的. 下面证明充分性. 即对于维数相等的空间存在同构映射(即满足线性映射的双射).<br>
设 <span class="mathjax-exps">\(V\)</span> 和 <span class="mathjax-exps">\(W\)</span> 均是数域 <span class="mathjax-exps">\(F\)</span> 上的 <span class="mathjax-exps">\(n\)</span> 维线性空间，向量组 <span class="mathjax-exps">\(\varepsilon_1, \cdots, \varepsilon_n\)</span> 和 <span class="mathjax-exps">\(\eta_1, \cdots, \eta_n\)</span> 分别是 <span class="mathjax-exps">\(V\)</span> 和 <span class="mathjax-exps">\(W\)</span> 的一组基。<br>
定义映射 <span class="mathjax-exps">\(T: V \rightarrow W\)</span> 满足<br>
</p><div class="mathjax-exps">\[T(x) = \sum_{i=1}^n \alpha_i \eta_i, \quad \forall x \in V\]</div> 式中：<span class="mathjax-exps">\(\alpha = [\alpha_1, \cdots, \alpha_n]^T\)</span> 是向量 <span class="mathjax-exps">\(x\)</span> 在基 <span class="mathjax-exps">\(\varepsilon_1, \cdots, \varepsilon_n\)</span> 下的坐标。<br>
由此，对任意向量 <span class="mathjax-exps">\(x, y \in V\)</span> 和 <span class="mathjax-exps">\(\lambda, \mu \in F\)</span>，有<br>
<div class="mathjax-exps">\[T(\lambda x + \mu y) = \sum_{i=1}^n (\lambda \alpha_i + \mu \beta_i) \eta_i = \lambda T(x) + \mu T(y)\]</div> 式中：<span class="mathjax-exps">\(\beta = [\beta_1, \cdots, \beta_n]^T\)</span> 是向量 <span class="mathjax-exps">\(y\)</span> 在基 <span class="mathjax-exps">\(\varepsilon_1, \cdots, \varepsilon_n\)</span> 下的坐标。上式表明 <span class="mathjax-exps">\(T\)</span> 是线性映射。若 <span class="mathjax-exps">\(T(x) = T(y)\)</span>，即<br>
<div class="mathjax-exps">\[T(x) - T(y) = \sum_{i=1}^n (\alpha_i - \beta_i) \eta_i = \theta\]</div> 则有 <span class="mathjax-exps">\(\alpha = \beta\)</span>，即 <span class="mathjax-exps">\(x = y\)</span>。于是当 <span class="mathjax-exps">\(x \neq y\)</span> 时，<span class="mathjax-exps">\(T(x) \neq T(y)\)</span>，这表明线性映射 <span class="mathjax-exps">\(T\)</span> 是单射。<br>
对任意向量 <span class="mathjax-exps">\(z \in W\)</span> 有<div class="mathjax-exps">\[z = \sum_{i=1}^n \gamma_i \eta_i\]</div> 式中：<span class="mathjax-exps">\(\gamma = [\gamma_1, \cdots, \gamma_n]^T\)</span> 是向量 <span class="mathjax-exps">\(z\)</span> 在基 <span class="mathjax-exps">\(\eta_1, \cdots, \eta_n\)</span> 下的坐标。<br>
定义<br>
<div class="mathjax-exps">\[\tilde{x} = \sum_{j=1}^n \gamma_j \varepsilon_j\]</div> 则有 <span class="mathjax-exps">\(\tilde{x} \in V\)</span>，且 <span class="mathjax-exps">\(T(\tilde{x}) = z\)</span>。这表明线性映射 <span class="mathjax-exps">\(T\)</span> 是满射。综上所述，映射 <span class="mathjax-exps">\(T\)</span> 是同构映射，即线性空间 <span class="mathjax-exps">\(V\)</span> 和 <span class="mathjax-exps">\(W\)</span> 是同构的。<p></p>
</details>
<h5 id="231-corollary"><em>2.3.1 Corollary</em> </h5>
<blockquote>
<p>任一实(复) n维线性空间均与 <span class="mathjax-exps">$\mathbb{R}^n(\mathbb{C}^n)$</span> 同构。</p>
</blockquote>
<h5 id="232-corollary"><em>2.3.2 Corollary</em> </h5>
<blockquote>
<p>设 <span class="mathjax-exps">$V$</span> 和 <span class="mathjax-exps">$W$</span> 是数域 <span class="mathjax-exps">$F$</span> 上的线性空间, 它们维数分别为 <span class="mathjax-exps">$n$</span> 和 <span class="mathjax-exps">$m$</span>, 则 <span class="mathjax-exps">$\operatorname{dim}(L(V, W))=\operatorname{dim}(F^m \times F^n)=mn$</span>。</p>
</blockquote>
<h5 id="233-corollary"><em>2.3.3 Corollary</em> </h5>
<blockquote>
<p>设 <span class="mathjax-exps">$V$</span> 是数域 <span class="mathjax-exps">$\mathbb{R}$</span> (或 <span class="mathjax-exps">$\mathbb{C}$</span>) 上的 n维线性空间, 则线性变换空间 <span class="mathjax-exps">$L(V)$</span> 与 <span class="mathjax-exps">$\mathbb{R}^{n^2}$</span> (或 <span class="mathjax-exps">$\mathbb{C}^{n^2}$</span>) 同构。</p>
</blockquote>
<h5 id="234-theorem-坐标变换"><em>2.3.4 Theorem: 坐标变换</em> </h5>
<blockquote>
<p>设<span class="mathjax-exps">$V$</span>和<span class="mathjax-exps">$W$</span>是数域<span class="mathjax-exps">$F$</span>上的线性空间，<span class="mathjax-exps">$T: V \to W$</span>是线性映射，<span class="mathjax-exps">$\epsilon _1, \cdots, \epsilon _n$</span>和<span class="mathjax-exps">$\eta _1, \cdots, \eta _m$</span>分别是<span class="mathjax-exps">$V$</span>和<span class="mathjax-exps">$W$</span>的基，<span class="mathjax-exps">$A$</span>是<span class="mathjax-exps">$T$</span>在这两组基下的矩阵，<span class="mathjax-exps">$x \in V$</span>的坐标为<span class="mathjax-exps">$\alpha$</span>，<span class="mathjax-exps">$T(x)$</span>在基<span class="mathjax-exps">$\eta _1, \cdots, \eta _m$</span>下的坐标为<span class="mathjax-exps">$\beta$</span>，则有<span class="mathjax-exps">$\beta = A\alpha$</span>.</p>
</blockquote>
<details open="">
  <summary>Proof:</summary> 
<p>必要性由命题 2.3.1性质(5)证得。这里只证明充分性。设 <span class="mathjax-exps">\(V\)</span> 和 <span class="mathjax-exps">\(W\)</span> 均是数域 <span class="mathjax-exps">\(F\)</span> 上的 <span class="mathjax-exps">\(n\)</span> 维线性空间，向量组 <span class="mathjax-exps">\(\varepsilon_1, \cdots, \varepsilon_n\)</span> 和 <span class="mathjax-exps">\(\eta_1, \cdots, \eta_n\)</span> 分别是 <span class="mathjax-exps">\(V\)</span> 和 <span class="mathjax-exps">\(W\)</span> 的一组基。定义映射 <span class="mathjax-exps">\(T: V \rightarrow W\)</span> 满足<br>
</p><div class="mathjax-exps">\[T(x) = \sum_{i=1}^n \alpha_i \eta_i, \quad \forall x \in V\]</div> 式中：<span class="mathjax-exps">\(\alpha = [\alpha_1, \cdots, \alpha_n]^T\)</span> 是向量 <span class="mathjax-exps">\(x\)</span> 在基 <span class="mathjax-exps">\(\varepsilon_1, \cdots, \varepsilon_n\)</span> 下的坐标。<br>
由此，对任意向量 <span class="mathjax-exps">\(x, y \in V\)</span> 和 <span class="mathjax-exps">\(\lambda, \mu \in F\)</span>，有<br>
<div class="mathjax-exps">\[T(\lambda x + \mu y) = \sum_{i=1}^n (\lambda \alpha_i + \mu \beta_i) \eta_i = \lambda T(x) + \mu T(y)\]</div> 式中：<span class="mathjax-exps">\(\beta = [\beta_1, \cdots, \beta_n]^T\)</span> 是向量 <span class="mathjax-exps">\(y\)</span> 在基 <span class="mathjax-exps">\(\varepsilon_1, \cdots, \varepsilon_n\)</span> 下的坐标。上式表明 <span class="mathjax-exps">\(T\)</span> 是线性映射。若 <span class="mathjax-exps">\(T(x) = T(y)\)</span>，即<br>
<div class="mathjax-exps">$$T(x) - T(y) = \sum_{i=1}^n (\alpha_i - \beta_i) \eta_i = \theta$$</div> 则有 <span class="mathjax-exps">\(\alpha = \beta\)</span>，即 <span class="mathjax-exps">\(x = y\)</span>。于是当 <span class="mathjax-exps">\(x \neq y\)</span> 时，<span class="mathjax-exps">\(T(x) \neq T(y)\)</span>，这表明线性映射 <span class="mathjax-exps">\(T\)</span> 是单射。<br>
对任意向量 <span class="mathjax-exps">\(z \in W\)</span> 有 <div class="mathjax-exps">\[z = \sum_{i=1}^n \gamma_i \eta_i\]</div> 式中：<span class="mathjax-exps">\(\gamma = [\gamma_1, \cdots, \gamma_n]^{\top}\)</span> 是向量 <span class="mathjax-exps">\(z\)</span> 在基 <span class="mathjax-exps">\(\eta_1, \cdots, \eta_n\)</span> 下的坐标。<br>
定义 <div class="mathjax-exps">\[\tilde{x} = \sum_{j=1}^n \gamma_j \varepsilon_j\]</div> 则有 <span class="mathjax-exps">\(\tilde{x} \in V\)</span>，且 <span class="mathjax-exps">\(T(\tilde{x}) = z\)</span>。这表明线性映射 <span class="mathjax-exps">\(T\)</span> 是满射。<br>
综上所述，映射 <span class="mathjax-exps">\(T\)</span> 是同构映射，即线性空间 <span class="mathjax-exps">\(V\)</span> 和 <span class="mathjax-exps">\(W\)</span> 是同构的。证毕。<p></p>
</details>
<p>下面这张图很好的解释了线性映射与矩阵的关系:</p>
<p align="center"><img src="./assert/图2-3-1.png" width="600"> </p>
<p>{注}:</p>
<ol>
<li>矩阵作用在向量坐标上等效于线性映射作用在原向量上.</li>
<li>在此可以抽象的把所有线性映射看作矩阵的乘法, 我们可以人为的规定不同的基, 从而全部规约到矩阵与向量的乘法, 所以在此之后只讨论矩阵与向量的乘法.</li>
<li>对于线性映射空间的一切性质都转换为对矩阵的研究.</li>
<li><span class="mathjax-exps">$m \times n$</span>阶矩阵与n维向量的乘法操作代表了某个n维线性空间中的向量正在经历线性变换。任何n维线性空间内的线性运算都可以等价为在<span class="mathjax-exps">$\mathbb{R}^n$</span>（或<span class="mathjax-exps">$\mathbb{C}^n$</span>）空间中的向量线性运算，因为n维线性空间与<span class="mathjax-exps">$\mathbb{R}^n$</span>或<span class="mathjax-exps">$\mathbb{C}^n$</span>是同构的。两个线性空间之间定义的线性映射可以等价为在<span class="mathjax-exps">$\mathbb{R}^{n \times n}$</span>（或<span class="mathjax-exps">$\mathbb{C}^{n \times n}$</span>）中的矩阵与<span class="mathjax-exps">$\mathbb{R}^n$</span>（或<span class="mathjax-exps">$\mathbb{C}^n$</span>）空间的向量进行乘法运算，即线性映射空间与其对应的矩阵空间是同构的。基于上述等价关系，我们可以将同构线性空间视为同一个线性空间。</li>
</ol>
<h5 id="235-theorem-基变换对线性映射对应矩阵的影响"><em>2.3.5 Theorem: 基变换对线性映射对应矩阵的影响</em> </h5>
<blockquote>
<p>设<span class="mathjax-exps">$V$</span>和<span class="mathjax-exps">$W$</span>是数域<span class="mathjax-exps">$F$</span>上的<span class="mathjax-exps">$n$</span>维和<span class="mathjax-exps">$m$</span>维线性空间，<span class="mathjax-exps">$\alpha_1, \alpha_2, \ldots, \alpha_n$</span>和<span class="mathjax-exps">$\beta_1, \beta_2, \ldots, \beta_m$</span>是<span class="mathjax-exps">$V$</span>的两组基，由<span class="mathjax-exps">$\alpha_1, \alpha_2, \ldots, \alpha_n$</span>到<span class="mathjax-exps">$\beta_1, \beta_2, \ldots, \beta_m$</span>的过渡矩阵为<span class="mathjax-exps">$Q$</span>；<span class="mathjax-exps">$\gamma_1, \gamma_2, \ldots, \gamma_m$</span>和<span class="mathjax-exps">$\delta_1, \delta_2, \ldots, \delta_m$</span>是<span class="mathjax-exps">$W$</span>的两组基，由<span class="mathjax-exps">$\gamma_1, \gamma_2, \ldots, \gamma_m$</span>到<span class="mathjax-exps">$\delta_1, \delta_2, \ldots, \delta_m$</span>的过渡矩阵为<span class="mathjax-exps">$P$</span>；设线性映射<span class="mathjax-exps">$T \in L(V, W)$</span>在<span class="mathjax-exps">$V$</span>的基<span class="mathjax-exps">$\alpha_1, \alpha_2, \ldots, \alpha_n$</span>和<span class="mathjax-exps">$W$</span>的基<span class="mathjax-exps">$\gamma_1, \gamma_2, \ldots, \gamma_m$</span>下的矩阵为<span class="mathjax-exps">$A$</span>，<span class="mathjax-exps">$T$</span>在<span class="mathjax-exps">$V$</span>的基<span class="mathjax-exps">$\beta_1, \beta_2, \ldots, \beta_n$</span>和<span class="mathjax-exps">$W$</span>的基<span class="mathjax-exps">$\delta_1, \delta_2, \ldots, \delta_m$</span>下的矩阵为<span class="mathjax-exps">$B$</span>，则<span class="mathjax-exps">$B = P^{-1}APQ$</span></p>
</blockquote>
<h5 id="234-corollary-基变换对线性变换对应矩阵的影响"><em>2.3.4 Corollary: 基变换对线性变换对应矩阵的影响</em> </h5>
<blockquote>
<p>设<span class="mathjax-exps">$V$</span>是数域<span class="mathjax-exps">$F$</span>上的<span class="mathjax-exps">$n$</span>维线性空间，<span class="mathjax-exps">$\varepsilon_1, \cdots, \varepsilon_n$</span>和<span class="mathjax-exps">$\varepsilon_1', \cdots, \varepsilon_n'$</span>是<span class="mathjax-exps">$V$</span>的两组基，由<span class="mathjax-exps">$\varepsilon_1, \cdots, \varepsilon_n$</span>到<span class="mathjax-exps">$\varepsilon_1', \cdots, \varepsilon_n'$</span>的过渡矩阵为<span class="mathjax-exps">$P$</span>，线性变换<span class="mathjax-exps">$T \in \mathcal{L}(V)$</span>在基<span class="mathjax-exps">$\varepsilon_1, \cdots, \varepsilon_n$</span>和基<span class="mathjax-exps">$\varepsilon_1', \cdots, \varepsilon_n'$</span>下的矩阵分别为<span class="mathjax-exps">$A$</span>和<span class="mathjax-exps">$B$</span>，则<span class="mathjax-exps">$B = P^{-1}AP$</span></p>
</blockquote>
<h5 id="236-theorem-线性映射的维数性质"><em>2.3.6 Theorem: 线性映射的维数性质</em> </h5>
<blockquote>
<p>设<span class="mathjax-exps">$V$</span>和<span class="mathjax-exps">$W$</span>是数域<span class="mathjax-exps">$F$</span>上的<span class="mathjax-exps">$n$</span>维和<span class="mathjax-exps">$m$</span>维线性空间，若<span class="mathjax-exps">$T \in \mathcal{L}(V, W)$</span>在<span class="mathjax-exps">$V$</span>的基<span class="mathjax-exps">$\varepsilon_1, \cdots, \varepsilon_n$</span>和<span class="mathjax-exps">$W$</span>的基<span class="mathjax-exps">$\eta_1, \cdots, \eta_m$</span>下的矩阵为<span class="mathjax-exps">$A$</span>，则:<br>
(1) <span class="mathjax-exps">$\operatorname{dim} N(T) = \operatorname{dim} N(A)$</span><br>
(2) <span class="mathjax-exps">$\operatorname{dim} R(T) = \operatorname{dim} R(A) = \operatorname{rank}(A)$</span>;<br>
(3) <span class="mathjax-exps">$\operatorname{dim} N(A) + \operatorname{dim} R(A) = n$</span>（秩-零度定理）</p>
</blockquote>
<h5 id="236-example"><em>2.3.6 Example</em> </h5>
<blockquote>
<p>考察齐次线性差分方程<br>
</p><div class="mathjax-exps">\[u_{k+n} + a_{n-1} u_{k+n-1} + \cdots + a_1 u_{k+1} + a_0 u_k = 0\]</div> <span class="mathjax-exps">\(k = 0, \pm 1, \pm 2, \cdots\)</span>。方程的解集 <span class="mathjax-exps">\(\widetilde{S}\)</span> 是 <span class="mathjax-exps">\(S\)</span> 的一个线性子空间。若定义<br>
<div class="mathjax-exps">\[x_k = \left[\begin{array}{c} u_k \\ u_{k+1} \\ \vdots \\ u_{k+n-1} \end{array}\right] \in \mathbb{R}^n, \quad A = \left[\begin{array}{ccccc} 0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1\\ -a_0 &amp; -a_1 &amp; a_2 &amp;...&amp; -a_{n-1}\end{array}\right] \in \mathbb{R}^{n \times n}\]</div> 则上式可改写为<br>
<div class="mathjax-exps">\[x_{k+1} = A x_k, \quad k = 0, \pm 1, \pm 2, \cdots\]</div> 由该式易验证，当 <span class="mathjax-exps">\(x_0\)</span> 给定，序列 <span class="mathjax-exps">\(\{ u_k \}\)</span> 唯一确定。因此，定义映射 <span class="mathjax-exps">\(T: \widetilde{S} \rightarrow \mathbb{R}^n\)</span> 满足<br>
<div class="mathjax-exps">\[T\left( \left\{ u_k \right\} \right) = x_0\]</div> 易证 <span class="mathjax-exps">\(T\)</span> 是同构映射。因此，<span class="mathjax-exps">\(\operatorname{dim}(\widetilde{S}) = \operatorname{dim}(\mathbb{R}^n) = n\)</span>，即 <span class="mathjax-exps">\(\widetilde{S}\)</span> 是 <span class="mathjax-exps">\(S\)</span> 的一个 <span class="mathjax-exps">\(n\)</span> 维线性子空间.<p></p>
</blockquote>
<h5 id="241-theorem-线性变换特征值特征向量"><em>2.4.1 Theorem: 线性变换特征值特征向量</em> </h5>
<blockquote>
<p>设线性变换<span class="mathjax-exps">$T \in L(V)$</span>,若存在 <span class="mathjax-exps">$\lambda_{0} \in F$</span> 及<span class="mathjax-exps">$V$</span>的非零向量<span class="mathjax-exps">$\xi$</span> 使得 <span class="mathjax-exps">$T(\xi) = \lambda_0 \xi$</span>, 则称 <span class="mathjax-exps">$\lambda_0$</span> 是 <span class="mathjax-exps">$T$</span>的一个特征值,称 <span class="mathjax-exps">$\xi$</span> 为 <span class="mathjax-exps">$T$</span>的属于特征值 <span class="mathjax-exps">$\lambda_0$</span> 的一个特征向量</p>
</blockquote>
<details open="">
  <summary>Proof:</summary>
</details>
<p>{注}: 特征向量一定为非零向量, 特征值可以为0.</p>
<h5 id="241-example-特征向量空间"><em>2.4.1 Example: 特征向量空间</em> </h5>
<blockquote>
<p><span class="mathjax-exps">$T$</span>的特征值<span class="mathjax-exps">$\lambda$</span>的所有特征向量组成的集合是线性空间吗?<br>
显然不是, 因为零向量不是特征向量.</p>
</blockquote>
<p>{注}: 设<span class="mathjax-exps">$T \in L(V)$</span>，<span class="mathjax-exps">$\{v_1, v_2, \ldots, v_n\}$</span>是<span class="mathjax-exps">$V$</span>的一组基，且<span class="mathjax-exps">$T(v_i) = \lambda_i v_i$</span>（<span class="mathjax-exps">$i = 1, 2, \ldots, n$</span>），则<span class="mathjax-exps">$T$</span>在基<span class="mathjax-exps">$\{v_1, v_2, \ldots, v_n\}$</span>下的矩阵为对角阵。</p>
<h5 id="242-definition-矩阵的特征值和特征向量"><em>2.4.2 Definition: 矩阵的特征值和特征向量</em> </h5>
<blockquote>
<p>设 <span class="mathjax-exps">$A\in F^{n\times n},\lambda$</span> 为一标量，矩阵 <span class="mathjax-exps">$\lambda I-A$</span> 称为<span class="mathjax-exps">$A$</span>的特征矩阵，其行列式 <span class="mathjax-exps">$|\lambda I-A|$</span> 称为<span class="mathjax-exps">$A$</span>的特征多项式，方程 <span class="mathjax-exps">$|\lambda I-A|=0$</span> 的根称为<span class="mathjax-exps">$A$</span>的特征值（或特征根）。方程 <span class="mathjax-exps">$(\lambda I-A)\alpha=0$</span> 的非零解向量<span class="mathjax-exps">$\alpha$</span>称为属于特征值 <span class="mathjax-exps">$\lambda$</span> 的特征向量。</p>
</blockquote>
<p>{注}:<br>
<span class="mathjax-exps">$\lambda$</span> 不一定为实数.</p>
<h5 id="24-theorem-线性变换特征值和矩阵特征值关系"><em>2.4.* Theorem: 线性变换特征值和矩阵特征值关系</em> </h5>
<blockquote>
<p><span class="mathjax-exps">$\lambda$</span>是线性变换<span class="mathjax-exps">$T$</span>的特征值当且仅当<span class="mathjax-exps">$\lambda$</span>是<span class="mathjax-exps">$A$</span>的特征值; 向量<span class="mathjax-exps">$\alpha$</span>是线性变换<span class="mathjax-exps">$T$</span>的特征向量当且仅当<span class="mathjax-exps">$\alpha$</span>是<span class="mathjax-exps">$A$</span>的特征向量，其中<span class="mathjax-exps">$A$</span>是<span class="mathjax-exps">$T$</span>在线性空间<span class="mathjax-exps">$V$</span>的基<span class="mathjax-exps">$\{v_1, v_2, \ldots, v_n\}$</span>下的矩阵表示, <span class="mathjax-exps">$\alpha$</span>和<span class="mathjax-exps">$\beta$</span>分别是向量<span class="mathjax-exps">$\alpha$</span>在基<span class="mathjax-exps">$\{v_1, v_2, \ldots, v_n\}$</span>下的坐标向量.</p>
</blockquote>
<details open="">
  <summary>Proof:</summary>
<blockquote>
<p>设 <span class="mathjax-exps">$T(x) =\lambda x$</span>, <span class="mathjax-exps">$\epsilon _1, \cdots, \epsilon _n$</span>为 <span class="mathjax-exps">$V$</span> 的一组基, <span class="mathjax-exps">$x = \sum_{i=1}^n \alpha_i \epsilon _i$</span>, 则有 <span class="mathjax-exps">$T(x) = \sum_{i=1}^n \alpha_i T(\epsilon _i) = (T(\epsilon_1), \cdots, T(\epsilon _n)) \cdot (\alpha _1, \cdots , \alpha _n)^\top = ( \epsilon _1, \cdots , \epsilon _n )A(\alpha _1, \cdots , \alpha _n)^\top = \lambda x = (\epsilon _1, \cdots, \epsilon _n)\lambda (\alpha _1, \cdots, \alpha _n)^\top$</span>, 即 <span class="mathjax-exps">$\lambda \alpha  = A \alpha$</span>, 其中 <span class="mathjax-exps">$A$</span> 是 <span class="mathjax-exps">$T$</span> 在基 <span class="mathjax-exps">$\epsilon _1, \cdots, \epsilon _n$</span> 下的矩阵表示, <span class="mathjax-exps">$\alpha$</span> 是 <span class="mathjax-exps">$x$</span> 在基 <span class="mathjax-exps">$\epsilon _1, \cdots, \epsilon _n$</span> 下的坐标向量. 证毕.</p>
</blockquote>
</details>
<p>{注}:矩阵 <span class="mathjax-exps">$A \in F^{n \times n}$</span>不一定有<span class="mathjax-exps">$n$</span>个特征值, 依赖于 <span class="mathjax-exps">$V$</span>所在的数域 <span class="mathjax-exps">$F$</span>.</p>
<h5 id="241-theorem-矩阵的迹与特征值"><em>2.4.1 Theorem: 矩阵的迹与特征值</em> </h5>
<blockquote>
<p>设 <span class="mathjax-exps">$\lambda_1,\cdots,\lambda_n$</span> 是矩阵 <span class="mathjax-exps">$A=\left(a_{i j}\right)\in C^{n\times n}$</span> 的特征值，则有：<br>
</p><div class="mathjax-exps">$$\prod_{i=1}^n\lambda_i=|A|,  \sum_{i=1}^n\lambda_i=\sum_{i=1}^n a_{i i}=\operatorname{tr}(A)$$</div><p></p>
</blockquote>
<h6 id="lemma-维达定理">Lemma: 维达定理 </h6>
<p>设 <span class="mathjax-exps">$P(x)=a_{n} x^{n}+a_{n-1} x^{n-1}+\cdots+a_{1} x+a_{0}$</span> 是一个一元 n 次实（或复）系数多项式，首项系数 <span class="mathjax-exps">$a_{n}\neq 0$</span>，令 P 的 n 个根为 <span class="mathjax-exps">$x_{1}, x_{2},\ldots, x_{n}$</span>，则根 <span class="mathjax-exps">$\left\{x_{i}\right\}$</span> 和系数 <span class="mathjax-exps">$\left\{a_{j}\right\}$</span> 之间满足关系式：<br>
</p><div class="mathjax-exps">$$\left\{ \begin{array}{l} x_{1}+x_{2}+\cdots+x_{n}=-\frac{a_{n-1}}{a_{n}} \\ \left(x_{1} x_{2}+x_{1} x_{3}+\cdots+x_{1} x_{n}\right)+\left(x_{2} x_{3}+x_{2} x_{4}+\cdots+x_{2} x_{n}\right)+\cdots+x_{n-1} x_{n}=\frac{a_{n-2}}{a_{n}} \\ \quad\vdots \\ x_{1} x_{2}\ldots x_{n}=(-1)^{n}\frac{a_{0}}{a_{n}} \end{array} \right.$$</div><p></p>
<details open="">
  <summary>Proof:</summary> 
<p>由代数基本定理有, <span class="mathjax-exps">$f(\lambda ) = |\lambda I-A| = \prod_{i=1}^{n}\left(\lambda-\lambda_{i}\right)=a_{n}\lambda^{n}+a_{n-1}\lambda^{n-1}+\cdots+a_{1}\lambda+a_{0}$</span><br>
显然只有 <span class="mathjax-exps">$|\lambda I-A|$</span>的对角线才能产生 <span class="mathjax-exps">$\lambda^n$</span> 和 <span class="mathjax-exps">$\lambda ^{n-1}$</span>的项. 关注特征多项式对角线, 即 <span class="mathjax-exps">$(\lambda -a_{11})\cdots (\lambda -a_{nn})$</span>, 显然 <span class="mathjax-exps">$\lambda ^n$</span> 的系数为1, <span class="mathjax-exps">$\lambda ^{n-1}$</span> 的系数为 <span class="mathjax-exps">$-(a_{11} + \cdots + a_{nn})$</span>, 所以 <span class="mathjax-exps">$\sum_{i=1}^n\lambda_i=\sum_{i=1}^n a_{i i}=\operatorname{tr}(A)$</span>.<br>
显然知 <span class="mathjax-exps">$f(\lambda )$</span> 的常数项为 <span class="mathjax-exps">$a_0 = f(0) = (-1)^n |A|$</span>, 所以 <span class="mathjax-exps">$\prod_{i=1}^n\lambda_i=|A|$</span>.</p>
</details>
<h5 id="244-example-特征值的性质"><em>2.4.4 Example: 特征值的性质</em> </h5>
<blockquote>
<p>设 <span class="mathjax-exps">$\lambda$</span> 是可逆复方阵 <span class="mathjax-exps">$A$</span> 的特征值, 试证明:<br>
(1)<span class="mathjax-exps">$\lambda^{-1}$</span> 是 <span class="mathjax-exps">$A^{-1}$</span> 的特征值;<br>
(2)<span class="mathjax-exps">$\lambda^{-1}A^{-1}$</span> 是 <span class="mathjax-exps">$A^*$</span> 的特征值.</p>
</blockquote>
<details open="">
  <summary>Proof:</summary>
<p>(1) 当 <span class="mathjax-exps">$A$</span> 可逆时，<span class="mathjax-exps">$\lambda \neq 0$</span>。令 <span class="mathjax-exps">$x$</span> 是属于特征值 <span class="mathjax-exps">$\lambda$</span> 的特征向量，则有：<br>
</p><div class="mathjax-exps">$$Ax = \lambda x$$</div><p></p>
<p>对上式左右两端乘以 <span class="mathjax-exps">$A^{-1}$</span>，并整理得：<br>
</p><div class="mathjax-exps">$$A^{-1}x = \lambda^{-1}x$$</div><p></p>
<p>故 <span class="mathjax-exps">$\lambda^{-1}$</span> 是 <span class="mathjax-exps">$A^{-1}$</span> 的特征值。<br>
(2) 根据 <span class="mathjax-exps">$AA^* = AI$</span> 知，<span class="mathjax-exps">$A^* \lambda AA^{-1} = \lambda I$</span>，故 <span class="mathjax-exps">$\lambda^{-1}A^{-1}$</span> 是 <span class="mathjax-exps">$A^*$</span> 的特征值。</p>
</details>
<h5 id="244-supplement-伴随矩阵"><em>2.4.4 Supplement: 伴随矩阵</em> </h5>
<blockquote>
<p>设 <span class="mathjax-exps">$A \in F^{n \times n}$</span>, 称矩阵 <span class="mathjax-exps">$A^* = \left( A_{i j} \right) \in F^{n \times n}$</span> 为 <span class="mathjax-exps">$A$</span> 的伴随矩阵, 其中 <span class="mathjax-exps">$A_{i j}$</span> 是 <span class="mathjax-exps">$A$</span> 的代数余子式, 即 <span class="mathjax-exps">$A_{i j} = (-1)^{i+j} M_{i j}$</span>, 其中 <span class="mathjax-exps">$M_{i j}$</span> 是 <span class="mathjax-exps">$A$</span> 的子式, <span class="mathjax-exps">$M_{i j}$</span> 是 <span class="mathjax-exps">$A$</span> 去掉第 <span class="mathjax-exps">$i$</span> 行和第 <span class="mathjax-exps">$j$</span> 列后得到的 <span class="mathjax-exps">$n-1$</span> 阶子式的行列式.</p>
</blockquote>
<p>性质:</p>
<ol>
<li><span class="mathjax-exps">$A A^* = A^* A = |A|I$</span></li>
<li><span class="mathjax-exps">$\operatorname{rank}\left(A^{*}\right)=\left\{\begin{array}{l}n,\operatorname{rank}(A)=n\\ 1,\operatorname{rank}(A)=n-1\\ 0,\operatorname{rank}(A)\leq n-2\end{array}\right.$</span></li>
</ol>
<details open="">
  <summary>Proof:</summary> 
<p>当 <span class="mathjax-exps">$\operatorname{rank}(A) = n$</span> 时, <span class="mathjax-exps">$A^* = |A|A^{-1}$</span>, 显然 <span class="mathjax-exps">$\operatorname{rank}(A^*) = n$</span>.<br>
当 <span class="mathjax-exps">$\operatorname{rank}(A) = n-1$</span> 时, 其只有 <span class="mathjax-exps">$n-1$</span> 个线性无关的列, 不妨设为前 <span class="mathjax-exps">$n-1$</span> 列, 显然只有 <span class="mathjax-exps">$A_{in} \neq 0$</span>, 所以 <span class="mathjax-exps">$\operatorname{rank}(A^*) = 1$</span>.<br>
当 <span class="mathjax-exps">$\operatorname{rank}(A) \leq n-2$</span> 时, 显然有 <span class="mathjax-exps">$A_{ij} = 0$</span>, 所以 <span class="mathjax-exps">$\operatorname{rank}(A^*) = 0$</span>.</p>
</details>
<h5 id="243-definition-特征子空间"><em>2.4.3 Definition: 特征子空间</em> </h5>
<blockquote>
<p>设 <span class="mathjax-exps">\(\lambda\)</span> 是矩阵 <span class="mathjax-exps">\(A \in \mathbb{C}^{n \times n}\)</span> 的一个特征值, 定义集合 <span class="mathjax-exps">\(E_{\lambda} = \{x \in \mathbb{C}^n | Ax = \lambda x\}\)</span>。则 <span class="mathjax-exps">\(E_{\lambda}\)</span> 是 <span class="mathjax-exps">\(\mathbb{C}^n\)</span> 的线性子空间, 称为属于特征值 <span class="mathjax-exps">\(\lambda\)</span> 的特征子空间, <span class="mathjax-exps">\(\dim(E_{\lambda})\)</span> 为特征值 <span class="mathjax-exps">\(\lambda\)</span> 的几何重数.</p>
</blockquote>
<p>{注}:</p>
<ol>
<li><span class="mathjax-exps">$E(\lambda )$</span> 中需要包括零向量.</li>
<li><span class="mathjax-exps">$\dim(E(\lambda )) = n - \operatorname{rank}(A-\lambda I)$</span></li>
<li>一元 <span class="mathjax-exps">$n$</span>次多项式在复数域内一定有 <span class="mathjax-exps">$n$</span> 个根, 其中<span class="mathjax-exps">$\lambda _i$</span> 为特征方程的重数, 称为代数重数.</li>
</ol>
<h5 id="242-theorem-特征值几何重数定理"><em>2.4.2 Theorem: 特征值几何重数定理</em> </h5>
<blockquote>
<p>复方阵的任一特征值的几何重数不超过它的代数重数.</p>
</blockquote>
<details open="">
  <summary>Proof:</summary> 
<p>设 <span class="mathjax-exps">$\lambda_0$</span> 为 <span class="mathjax-exps">$n$</span> 阶矩阵 <span class="mathjax-exps">$A \in \mathbb{C}^{n \times n}$</span> 的一个特征值，其代数重数和几何重数分别为 <span class="mathjax-exps">$m$</span> 和 <span class="mathjax-exps">$k$</span>。由此，设 <span class="mathjax-exps">$p_1, \cdots, p_k$</span> 是特征子空间 <span class="mathjax-exps">$E(\lambda_0)$</span> 的一组基。由基扩充定理知，可将它扩充为 <span class="mathjax-exps">$\mathbb{C}^n$</span> 的一组基，记为 <span class="mathjax-exps">$p_1, \cdots, p_k, p_{k+1}, \cdots, p_n$</span>。<br>
定义 <span class="mathjax-exps">$P=\left[p_1,\cdots, p_k, p_{k+1},\cdots, p_n\right]\in \mathbb{C}^{n\times n}$</span><br>
</p><div class="mathjax-exps">$$P^{-1} A P=\left[\begin{array}{ll}\Lambda_0&amp; B_1\\ 0&amp; B_2\end{array}\right]\stackrel{\text{def}}{=} B$$</div> 式中：<span class="mathjax-exps">$\Lambda_0$</span> 为以 <span class="mathjax-exps">$\lambda_0$</span> 为对角元素的 <span class="mathjax-exps">$k$</span> 阶矩阵，<span class="mathjax-exps">$B_2$</span> 为 <span class="mathjax-exps">$(n-k)$</span> 阶矩阵。<br>
由于 <span class="mathjax-exps">$A$</span> 与 <span class="mathjax-exps">$B$</span> 相似，故它们有相同的特征值。注意到 <span class="mathjax-exps">$B$</span> 的特征多项式为<br>
<div class="mathjax-exps">$$\left|\lambda I_n-B\right|=\left(\lambda-\lambda_0\right)^k\left|\lambda I_{n-k}-B_2\right|$$</div> 即 <span class="mathjax-exps">$\lambda_0$</span> 在 <span class="mathjax-exps">$B$</span> 中的代数重数至少为 <span class="mathjax-exps">$k$</span>，故它在 <span class="mathjax-exps">$A$</span> 中的代数重数也至少为 <span class="mathjax-exps">$k$</span>，于是有 <span class="mathjax-exps">$m\geqslant k$</span>。证毕。<p></p>
</details>
<h5 id="241-proposition-相似矩阵的性质"><em>2.4.1 Proposition: 相似矩阵的性质</em> </h5>
<blockquote>
<p>若<span class="mathjax-exps">\(n\)</span>阶方阵<span class="mathjax-exps">\(A\)</span>与<span class="mathjax-exps">\(B\)</span>相似, 即存在可逆矩阵 <span class="mathjax-exps">$P$</span>, 使得<span class="mathjax-exps">$P^{-1}BP=A$</span> 则：<br>
(1) <span class="mathjax-exps">\(A\)</span>与<span class="mathjax-exps">\(B\)</span>有相同的特征多项式与特征值；<br>
(2) <span class="mathjax-exps">\(A\)</span>与<span class="mathjax-exps">\(B\)</span>有相同的秩与行列式；<br>
(3) <span class="mathjax-exps">\(A\)</span>与<span class="mathjax-exps">\(B\)</span>有相同的迹。</p>
</blockquote>
<details open="">
  <summary>Proof:</summary> 
<p>(1) <span class="mathjax-exps">$由 Ax = \lambda x, 得 P^{-1}BPx = \lambda x$</span>, 即 <span class="mathjax-exps">$B(Px) = \lambda (Px)$</span>, 显然特征值相同, 则特征多项式也相同.<br>
(2) 可逆矩阵都可以表示为初等矩阵的乘积, 初等矩阵不改变矩阵的秩, 所以 <span class="mathjax-exps">$A, B$</span> 秩相同.<br>
(3) 显然迹为特征值之和, 所以迹相同.</p>
</details>
<p>{注}:</p>
<ol>
<li>线性变换的矩阵的特征多项式与基的选取无关，它直接由线性变换决定，故可称之为线性变换的特征多项式.</li>
<li>显然相似矩阵的特征向量可以不同.</li>
</ol>
<h5 id="243-theorem-特征向量的线性无关性"><em>2.4.3 Theorem: 特征向量的线性无关性</em> </h5>
<blockquote>
<p>矩阵<span class="mathjax-exps">\(A\)</span>的属于不同特征值的特征向量线性无关。</p>
</blockquote>
<details open="">
  <summary>Proof:</summary> 
<p>设<span class="mathjax-exps">\(\lambda_1, \cdots, \lambda_r\)</span>是<span class="mathjax-exps">\(n\)</span>阶矩阵<span class="mathjax-exps">\(A\)</span>的<span class="mathjax-exps">\(r\)</span>个互不相同的特征值，<span class="mathjax-exps">\(\alpha_1, \cdots, \alpha_r\)</span>是分别属于特征值<span class="mathjax-exps">\(\lambda_1, \cdots, \lambda_r\)</span>的特征向量。<br>
考察向量方程<br>
</p><div class="mathjax-exps">\[k_1\alpha_1 + \cdots + k_r\alpha_r = 0 \qquad (2.4.3)\]</div> 式中<span class="mathjax-exps">\(k_1, \cdots, k_r \in F\)</span>为待定系数。<br>
对式(2.4.3)两端左乘矩阵<span class="mathjax-exps">\(A, A^2, \cdots, A^{n-1}\)</span>，得如下方程组：<br>
<div class="mathjax-exps">\[\begin{gathered} k_1\lambda_1\alpha_1 + \cdots + k_r\lambda_r\alpha_r = 0 \\ k_1\lambda_1^2\alpha_1 + \cdots + k_r\lambda_r^2\alpha_r = 0 \\ \vdots \\ k_1\lambda_1^{n-1}\alpha_1 + \cdots + k_r\lambda_r^{n-1}\alpha_r = 0 \end{gathered}\]</div> 联合式(2.4.3)，有<br>
<div class="mathjax-exps">\[\left[k_1\alpha_1, \cdots, k_r\alpha_r\right] D^{\top} = 0 \qquad (2.4.4)\]</div> 其中<br>
<div class="mathjax-exps">\[D = \left[\begin{array}{cccc} 1 &amp; 1 &amp; \cdots &amp; 1 \\ \lambda_1 &amp; \lambda_2 &amp; \cdots &amp; \lambda_r \\ \vdots &amp; \vdots &amp; &amp; \vdots \\ \lambda_1^{n-1} &amp; \lambda_2^{n-1} &amp; \cdots &amp; \lambda_r^{n-1} \end{array}\right]\]</div> 由于<span class="mathjax-exps">\(|D| = \prod_{i &lt; j} (\lambda_i - \lambda_j)\)</span>，知<span class="mathjax-exps">\(D\)</span>为可逆矩阵。对式(2.4.4)两端右乘矩阵<span class="mathjax-exps">\((D^\top )^{-1}\)</span>，得<br>
<div class="mathjax-exps">\[\left[k_1\alpha_1, \cdots, k_r\alpha_r\right] = 0\]</div> 由于<span class="mathjax-exps">\(\alpha_1, \cdots, \alpha_r \neq 0\)</span>，故<span class="mathjax-exps">\(k_1 = k_2 = \cdots = k_r = 0\)</span>。因此，矩阵<span class="mathjax-exps">\(A\)</span>的属于不同特征值的特征向量线性无关。<p></p>
</details>
<h5 id="251-definition-正交变换和酉变换"><em>2.5.1 Definition: 正交变换和酉变换</em> </h5>
<blockquote>
<p>若欧氏（酉）空间中的线性变换<span class="mathjax-exps">\(T\)</span>保持向量的内积不变，即对<span class="mathjax-exps">\(V\)</span>的任意向量<span class="mathjax-exps">\(x\)</span>与<span class="mathjax-exps">\(y\)</span>有<br>
</p><div class="mathjax-exps">\[(T(x), T(y)) = (x, y)\]</div> 则称<span class="mathjax-exps">\(T\)</span>为正交（酉）变换。<p></p>
</blockquote>
<h5 id="252-definition-正交矩阵和酉矩阵"><em>2.5.2 Definition: 正交矩阵和酉矩阵</em> </h5>
<blockquote>
<p>若<span class="mathjax-exps">\(n\)</span>阶实方阵<span class="mathjax-exps">\(A\)</span>满足<span class="mathjax-exps">\(A^T A = I\)</span>或<span class="mathjax-exps">\(A A^T = I\)</span>，则称<span class="mathjax-exps">\(A\)</span>为正交矩阵；若<span class="mathjax-exps">\(n\)</span>阶复方阵<span class="mathjax-exps">\(A\)</span>满足<span class="mathjax-exps">\(A^H A = I\)</span>或<span class="mathjax-exps">\(A A^H = I\)</span>，则称<span class="mathjax-exps">\(A\)</span>为酉矩阵。</p>
</blockquote>
<h5 id="251-theorem-正交酉变换的性质"><em>2.5.1 Theorem: 正交(酉)变换的性质</em> </h5>
<blockquote>
<p>设<span class="mathjax-exps">\(V\)</span>是<span class="mathjax-exps">\(n\)</span>维欧氏（酉）空间，<span class="mathjax-exps">\(T \in L(V)\)</span>，则以下命题等价：<br>
(1) <span class="mathjax-exps">\(T\)</span>是正交（酉）变换；<br>
(2) <span class="mathjax-exps">\(T\)</span>保持长度不变，即<span class="mathjax-exps">\(\|T(x)\| = \|x\|\)</span>；<br>
(3) 若<span class="mathjax-exps">\(\xi_1, \cdots, \xi_n\)</span>是<span class="mathjax-exps">\(V\)</span>中一组标准正交基，则<span class="mathjax-exps">\(T(\xi_1), \cdots, T(\xi_n)\)</span>也是<span class="mathjax-exps">\(V\)</span>中一组标准正交基；<br>
(4) <span class="mathjax-exps">\(T\)</span>在<span class="mathjax-exps">\(V\)</span>的任一标准正交基下的矩阵<span class="mathjax-exps">\(A\)</span>为正交（酉）矩阵。</p>
</blockquote>
<p>{注}: 正交矩阵 <span class="mathjax-exps">$A$</span> 的特征值不一定为<span class="mathjax-exps">$\pm1$</span>, 有可能是复数.</p>
<details open="">
  <summary>Proof:</summary> 
<p>设 <span class="mathjax-exps">$\lambda$</span> 是 <span class="mathjax-exps">$A$</span> 的任一特征值，<span class="mathjax-exps">$x$</span> 是属于 <span class="mathjax-exps">$\lambda$</span> 的特征向量, 则有 <span class="mathjax-exps">$Ax = \lambda x$</span>, 两端取转置, 得 <span class="mathjax-exps">$x^H A^H = \bar{\lambda}x^H$</span>,等式两端分别相乘得 <span class="mathjax-exps">$x^H A^H A x = \bar{\lambda} \lambda x^H x$</span> 由 <span class="mathjax-exps">$A^H A = I$</span> 得， <span class="mathjax-exps">$x^H x = \bar{\lambda } x^H x$</span>, 则 <span class="mathjax-exps">$\|\lambda\|^2 = 1$</span></p>
</details>
<h5 id="251-proposition-正交酉矩阵性质"><em>2.5.1 Proposition: 正交（酉）矩阵性质</em> </h5>
<blockquote>
<p>(1) 正交矩阵的行列式必为 <span class="mathjax-exps">$\pm 1$</span>，酉矩阵的行列式的模值为 <span class="mathjax-exps">$1$</span>。<br>
(2) <span class="mathjax-exps">$A^{-1} = A^H$</span> 均为正交（酉）矩阵。<br>
(3) 正交（酉）矩阵的乘积仍为正交（酉）矩阵。<br>
(4) <span class="mathjax-exps">$A$</span> 的所有特征值的模值为 <span class="mathjax-exps">$1$</span>。</p>
</blockquote>
<details open="">
  <summary>Proof:</summary> 
<p>(1) <span class="mathjax-exps">$|AA^H| = |A||A^H| = |A|^2 = 1$</span><br>
(2) 显然成立.<br>
(3) 设有酉矩阵 <span class="mathjax-exps">$B$</span>, 则 <span class="mathjax-exps">$(AB)^H(AB) = (B^HA^H)(AB) = B^H(A^HA)B = I$</span>.<br>
(4) 证明见上.</p>
</details>
<h5 id="252-theorem-正交酉矩阵条件"><em>2.5.2 Theorem: 正交（酉）矩阵条件</em> </h5>
<blockquote>
<p>矩阵<span class="mathjax-exps">\(A\)</span>是<span class="mathjax-exps">\(n\)</span>阶正交（酉）矩阵当且仅当矩阵<span class="mathjax-exps">\(A\)</span>的<span class="mathjax-exps">\(n\)</span>个列（行）向量构成<span class="mathjax-exps">\(n\)</span>维欧氏（酉）空间的一组标准正交基。</p>
</blockquote>
<h5 id="252-proposition-givens矩阵的性质"><em>2.5.2 Proposition: Givens矩阵的性质</em> </h5>
<p></p><div class="mathjax-exps">\[(t_{kl}(i, j))_{n \times n} = \begin{array}{ccccc} \cos\varphi &amp; 0 &amp; \cdots &amp; 0 &amp; \sin\varphi \\  &amp; 1 &amp; 0 &amp;&amp; 0 \\ \vdots &amp;&amp; \ddots &amp;&amp; \vdots \\ 0 &amp;&amp;&amp; 1 &amp; 0 \\ -\sin\varphi &amp; 0 &amp; \cdots &amp; 0 &amp; \cos\varphi \\ \end{array}\]</div><p></p>
<p>其中：<span class="mathjax-exps">\(t_{ii}(i, j) = t_{jj}(i, j) = \cos\varphi,\)</span> <span class="mathjax-exps">\(t_{ij}(i, j) = \sin\varphi,\)</span>, <span class="mathjax-exps">\(t_{ji}(i, j) = -\sin\varphi\)</span>. 对于 <span class="mathjax-exps">\(k \neq i, j\)</span>, <span class="mathjax-exps">\(t_{kk}(i, j) = 1\)</span>，并且对于任意 <span class="mathjax-exps">\(k \neq i, j\)</span> 和 <span class="mathjax-exps">\(l \neq i, j\)</span>, <span class="mathjax-exps">\(t_{kl}(i, j) = 0\)</span>。 矩阵 <span class="mathjax-exps">\(T(i, j)\)</span> 被称为Givens矩阵(或初等旋转矩阵)。</p>
<blockquote>
<p>设Givens矩阵<span class="mathjax-exps">\(T(i, j) \in \mathbb{R}^{n \times n}\)</span>，则以下命题成立：<br>
(1) <span class="mathjax-exps">\(T(i, j)\)</span>是正交矩阵且<span class="mathjax-exps">\((T(i, j))^{-1} = (T(i, j))^{\top}\)</span>.<br>
(2) 设<span class="mathjax-exps">\(x = \left[x_1, \cdots, x_n\right]^{\top}\)</span>，若<span class="mathjax-exps">\(y = T(i, j)x = \left[y_1, \cdots, y_n\right]^{\top}\)</span>，则<br>
</p><div class="mathjax-exps">\[y_k = x_k, \quad k \neq i, j\]</div><div class="mathjax-exps">\[y_i = \cos\varphi x_i + \sin\varphi x_j\]</div><div class="mathjax-exps">\[y_j = -\sin\varphi x_i + \cos\varphi x_j\]</div><p></p>
</blockquote>
<details open="">
  <summary>Proof:</summary> 
<p>(1) 由 <span class="mathjax-exps">$t_{kl}(i, j)^\top t_{kl}(i, j) = I$</span> 可知.<br>
(2) 显然成立.</p>
</details>
<p>{注}: 若 <span class="mathjax-exps">$\sqrt[]{x_i^2+x_j^2 }\neq 0$</span>, 可定义<br>
</p><div class="mathjax-exps">\[\cos\varphi = \frac{x_i}{\sqrt{x_i^2 + x_j^2}}, \quad \sin\varphi = \frac{x_j}{\sqrt{x_i^2 + x_j^2}}\]</div><p></p>
<p>则有 <span class="mathjax-exps">$y_i=\sqrt[]{x_i^2 + x_j^2}, y_j = 0$</span>, 显然可以经过有限次Givens变换 <span class="mathjax-exps">$T$</span>使得 <span class="mathjax-exps">$Tx=\|x\|e_1$</span>.</p>
<h5 id="253-definition-householder矩阵"><em>2.5.3 Definition: Householder矩阵</em> </h5>
<blockquote>
<p>设<span class="mathjax-exps">\(w \in \mathbb{C}^n\)</span>是单位向量，定义矩阵</p><div class="mathjax-exps">\[H(w) = I - 2ww^H\]</div>称为Householder矩阵（或初等反射矩阵）。<p></p>
</blockquote>
<p>{注}:<br>
对于二维平面, 设 <span class="mathjax-exps">$w$</span> 为单位向量, 如下图所示.<br>
有 <span class="mathjax-exps">$x+2p = y$</span>, <span class="mathjax-exps">$x + p = Proj_{W^\perp }x = x - (x,w)w$</span><br>
则有 <span class="mathjax-exps">$y = x - 2ww^Hx = H(x)w$</span></p>
<p align="center"><img src="./assert/图2-5-1.png" width="600"> </p>
<h5 id="253-proposition-householder矩阵的性质"><em>2.5.3 Proposition: Householder矩阵的性质</em> </h5>
<blockquote>
<p>Householder矩阵<span class="mathjax-exps">\(H(w)\)</span>具有以下性质：<br>
<span class="mathjax-exps">\((1) |H(w)| = -1\)</span>.<br>
(2) <span class="mathjax-exps">\((H(w))^H = H(w) = (H(w))^{-1}\)</span>；<br>
(3) 设 <span class="mathjax-exps">\(x, y \in \mathbb{C}^n\)</span> 且 <span class="mathjax-exps">\(x \neq y\)</span>，则存在单位向量 <span class="mathjax-exps">\(w\)</span> 使得 <span class="mathjax-exps">\(H(w)x = y\)</span> 的充分必要条件是： </p><div class="mathjax-exps">\[x^H x = y^H y, \quad x^H y = y^H x\]</div> 并且若上述条件成立，则使 <span class="mathjax-exps">\(H(w)x = y\)</span> 成立的单位向量 <span class="mathjax-exps">\(w\)</span> 可取为： <div class="mathjax-exps">\[w = \frac{e^{i\theta}}{\|x-y\|}(x-y)\]</div> 其中 <span class="mathjax-exps">\(\theta\)</span> 为任一实数。<p></p>
</blockquote>
<details open="">
  <summary>Proof:</summary> 
<p>(1) 显然 <span class="mathjax-exps">$H(w)w = (I-2ww^H)w = w - 2ww^Hw = -w$</span>, 则 <span class="mathjax-exps">$\lambda = -1$</span> 为一个特征值, <span class="mathjax-exps">$w$</span> 为一个特征向量.<br>
<span class="mathjax-exps">$\forall y \in W ^\perp$</span>, <span class="mathjax-exps">$H(w)y = (I-2ww^H)y = y - 2ww^Hy = y$</span>, 显然 <span class="mathjax-exps">$\lambda = 1$</span> 为一个特征值, <span class="mathjax-exps">$y$</span> 为特征向量. 由于几何重数一定小于等于代数重数, 所以 <span class="mathjax-exps">$\lambda = \pm 1$</span>为所有的特征值, 所以 <span class="mathjax-exps">$|H(w)| = -1$</span>.<br>
(2) <span class="mathjax-exps">$(H(w))^H = (I-2ww^H)^H = I - 2ww^H = H(w)$</span><br>
<span class="mathjax-exps">$H(w)(H(w))^H = H(w)H(w) = (I - 2ww^H)(I - 2ww^H) = I - 2ww^H - 2ww^H + 4ww^Hww^H = I$</span>. <span class="mathjax-exps">$(H(w))^H = (H(w))^{-1}$</span><br>
(3)<br>
必要性.<br>
巧用 <span class="mathjax-exps">$w^H w = 1$</span><br>
<span class="mathjax-exps">$y^H y = (x^H - 2x^Hww^H)(x - 2ww^Hx) = x^Hx - 2x^Hww^Hx - 2x^Hww^Hx + 4x^Hww^Hx = x^Hx$</span><br>
<span class="mathjax-exps">$\begin{aligned} x^Hy &amp;= x^H(I - 2ww^H)x \\ &amp;= x^Hx - 2x^Hww^Hx \\ &amp;= y^Hy - 2x^Hww^Hx \\ &amp;= y^H(I - 2ww^H)x - 2x^Hww^Hx \\ &amp;= y^Hx - 2y^Hww^Hx - 2x^Hww^Hx \\ &amp;= y^Hx - 2x^H (I-2ww^H)ww^Hx - 2x^Hww^Hx \\ &amp;= y^Hx - 2x^Hww^Hx + 4x^Hww^Hww^Hx - 2x^Hww^Hx \\  &amp;= y^Hx \end{aligned}$</span><br>
充分性.<br>
已知 <span class="mathjax-exps">$x^Hx = y^Hy, x^Hy = y^Hx$</span>.<br>
显然 <span class="mathjax-exps">$w = \frac{e^{i\theta}}{\|x-y\|}(x-y)$</span> 和 <span class="mathjax-exps">$w^\perp  = \frac{e^{i\theta}}{\|x+y\|}(x+y)$</span> 正交.<br>
则取 <span class="mathjax-exps">$p = \frac{1}{2}(y - x)$</span>, <span class="mathjax-exps">$x + p = \frac{1}{2} (y + x)$</span>, 则可验证 <span class="mathjax-exps">$x, p, x + p$</span> 构成直角三角形. 显然, <span class="mathjax-exps">$p = -Proj_{w}x$</span><br>
则 <span class="mathjax-exps">$y = x + 2p = x - 2w(x,w) = x - 2 ww^Hx = I(w)x$</span>. 证毕.</p>
</details>
<h5 id="253-example"><em>2.5.3 Example</em> </h5>
<blockquote>
<p>给定实方阵 <span class="mathjax-exps">$A$</span>, 是否存在有限个Givens矩阵或Householder矩阵的乘积, 记为 <span class="mathjax-exps">$T$</span>, 使得 <span class="mathjax-exps">$TA$</span> 变成如下形式：<br>
</p><div class="mathjax-exps">$$U = \left[\begin{array}{cccc} \lambda_1 &amp; * &amp; \cdots &amp; * \\ 0 &amp; \lambda_2 &amp; \cdots &amp; \vdots \\ \vdots &amp; \cdots &amp; \ddots &amp; * \\ 0 &amp; \cdots &amp; 0 &amp; \lambda_n \end{array}\right]$$</div><br>
其中, *为任意实数，<span class="mathjax-exps">$\lambda_1, \cdots, \lambda_n$</span>是<span class="mathjax-exps">$n$</span>个实数.<p></p>
</blockquote>
<details open="">
  <summary>Proof:</summary>
<p>对于 <span class="mathjax-exps">$T(i,j)A = T(i,j)\left[a_1, \cdots ,a_n\right]$</span> 可以选取 <span class="mathjax-exps">$T(i,j)$</span> 使得 <span class="mathjax-exps">$a_i$</span> 的第 <span class="mathjax-exps">$i$</span> 个分量为 <span class="mathjax-exps">$\sqrt[]{a_{ii}^2 + a_{ij}^2}$</span>, 第 <span class="mathjax-exps">$j$</span> 个分量为0. 则令 <span class="mathjax-exps">$T_i$</span> 为将 <span class="mathjax-exps">$a_i$</span> 的第 <span class="mathjax-exps">$i$</span> 个分量变为 <span class="mathjax-exps">$\sqrt[]{a_{ii}^2 + \cdots a_{in}^2}$</span>, 第 <span class="mathjax-exps">$j &gt; i$</span> 个分量为0 的Givens矩阵. 令 <span class="mathjax-exps">$T = T_n \cdots T_1$</span>, 则 <span class="mathjax-exps">$TA = U$</span>.<br>
Householder矩阵 可以实现相同的功能.</p>
</details>

      </div>
      
      
    
    
    
    
    
    
  
    </body></html>